{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4368ea-3569-44db-990d-f2bc4ced8250",
   "metadata": {},
   "source": [
    "# Exercise 3: More Advanced AD with Convolutional Auto-Encoders, Non-Standard Losses, And Generative Adversarial Network Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10933f-724b-4837-90f1-81494e1effd8",
   "metadata": {},
   "source": [
    "### Goals of the Exercise\n",
    "\n",
    "- Make a 2D convolutional autoencoder for events\n",
    "- Explore some non-standard losses for 2D images/autoencoders\n",
    "- Create a Generative Adversarial Network for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aff670-c1f8-4d6d-bf68-5f832fd0fc29",
   "metadata": {},
   "source": [
    "### More Anomaly Detection With Autoencoders\n",
    "\n",
    "A very common form of data in particle physics (and everything else) is images. 2D maps of the visual of something. There are, of course, autoencoders for this, \"convolutional autoencoders\", that use the same filter based features as a traditional image classifier convolutional neural network.\n",
    "\n",
    "Very often, data comes in 2D grids (especially for something like a detector or calorimeter), or a 2D image can be made by imposing a grid based structure on spatial data (note: this is acutally a bit non-ideal as it can lead to sparse data representations, and a whole lot of math on a whole lot of nothing, a problem that graph auto-encoders in the next exercise solve).\n",
    "\n",
    "at this point, you know the drill, we're going to make some auto-encoders for this, and see how they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f02ca4-8fbe-4202-a06a-d085fb508949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:32:38.903413: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749843158.926230 2160825 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749843158.933386 2160825 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749843158.952778 2160825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749843158.952805 2160825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749843158.952808 2160825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749843158.952811 2160825 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-13 14:32:38.958552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69954</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m69954\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65013</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m65013\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88923</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m88923\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">204360</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m204360\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m999\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "from rich.console import Console\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def load_data(file_path: str, limit_entries: int = -1) -> np.array:\n",
    "    with h5py.File(file_path) as the_file:\n",
    "        data = np.array(the_file['event_image_data'])[:limit_entries]\n",
    "    return data\n",
    "\n",
    "nprng = np.random.default_rng(42)\n",
    "\n",
    "#Perform random phi rotations on a dataset.\n",
    "#Assumes phi index per example is the first index\n",
    "def random_phi_rotations(the_data:np.array, max_shift: int =8) -> np.array:\n",
    "    new_examples = []\n",
    "    for example in the_data:\n",
    "        shift_value = nprng.integers(-max_shift, max_shift+1)\n",
    "        phi_shifted_example = np.roll(example, axis=0, shift=shift_value)\n",
    "        new_examples.append(phi_shifted_example)\n",
    "    new_examples = np.array(new_examples)\n",
    "    return new_examples\n",
    "\n",
    "#perform random eta reflections on a dataset\n",
    "#Assume eta index per example is the second index\n",
    "def random_eta_reflections(the_data: np.array)->np.array:\n",
    "    new_examples = []\n",
    "    for example in the_data:\n",
    "        if nprng.random() >= 0.5:\n",
    "            new_example = example[:, ::-1]\n",
    "        else:\n",
    "            new_example = example\n",
    "        new_examples.append(new_example)\n",
    "    new_examples = np.array(new_examples)\n",
    "    return new_examples\n",
    "\n",
    "zerobias_data = random_eta_reflections(random_phi_rotations(load_data('data/advanced_files/ZeroBiasAdvancedData.h5')))\n",
    "\n",
    "console.print(zerobias_data.shape)\n",
    "\n",
    "jetht_data = random_eta_reflections(random_phi_rotations(load_data('data/advanced_files/JetHTAdvancedData.h5')))\n",
    "\n",
    "console.print(jetht_data.shape)\n",
    "\n",
    "ttbar_data = random_eta_reflections(random_phi_rotations(load_data('data/advanced_files/TTBarAdvancedData.h5')))\n",
    "\n",
    "console.print(ttbar_data.shape)\n",
    "\n",
    "softqcd_data = random_eta_reflections(random_phi_rotations(load_data('data/advanced_files/SoftQCDAdvancedData.h5')))\n",
    "\n",
    "console.print(softqcd_data.shape)\n",
    "\n",
    "radion_data = random_eta_reflections(random_phi_rotations(load_data('data/advanced_files/RadionAdvancedData.h5')))\n",
    "\n",
    "console.print(radion_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2057bf-9328-48b1-905a-42934a9412ad",
   "metadata": {},
   "source": [
    "The data I have provided here is a $16\\times16$ grid of all reconstructed CMS jets, electrons, photons, muons and taus. The features, per grid cell are:\n",
    "\n",
    "0. $p_{T}$\n",
    "1. $\\phi$\n",
    "2. $\\eta$\n",
    "3. $m$\n",
    "4. A simple code to denote what type of object it is. 1 for jet, 2 for electron, 3 for photon, 4 for muon, 5 for tau.\n",
    "\n",
    "I have also done some dataset augmentation. I have randomly rotated each image in phi so that our autoencoder does not become too dependent on a specific angular setup. It, and physics should not be dependent on that. I have also randomly reflected some examples across eta. The physics here is dependent on eta, but not on the reflection, and the autoencoder should be invariant to that as well.\n",
    "\n",
    "We're going to make a convolutional 2D auto-encoder. There are a lot of things one can do to potentially tune and make an auto-encoder like this. This section is going to be a bit more free-form and I am going to encourage you to try playing around with a bunch of them.\n",
    "\n",
    "Some ideas to look at:\n",
    "- In addition to the latent space _size_ consider whether the latent space is _flat_. One could, in theory completely flatten out the latent space encoding via a `Flatten` layer... or via other methods with less location dependence like Global Pooling. Check how that works in the loss, and for anomaly detection. If you don't flatten it, you can instead maintain a 2D structure, but reduce the amount of information available, and later expand it outward again. Try that, alongside things like max or average pooling.\n",
    "- Some may be wondering \"how do I go from 2D data to a smaller or flat latent space, then back out?\" [keras has reshape layers](https://keras.io/api/layers/reshaping_layers/reshape/) that can allow you to make your completely flat data 2D (or any other shape) again. If you're wondering how to go from a smaller 2D image to a larger one, a simple method is [upsampling](https://keras.io/api/layers/reshaping_layers/up_sampling2d/) which will take a cell and duplicate it next to itself. A likely better solution are [transpose convolutional layers](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/), which behave as a convolutional layer, but in reverse, becoming in effect, learnable upsampling\n",
    "- Convolutional layers have a kernel size, i.e. how many cells worth of features in each direction it uses in its filter creation. Play around with the size of those and see if it makes a difference.\n",
    "- Similarly, Convolutional layers can have stride, i.e. how many cells it skips when it makes another filter calculation. Do be aware, this will change the shape when you use stride\n",
    "- Convolutional neural networks can also have padding, typically \"valid\" padding, only using filled cells (this will also change the output image size), or \"same\" padding (the image is padded with empty cells until the output image would be the same size as the original input image. Play around with those and see where it goes.\n",
    "\n",
    "I'd like to hear from some people what the best overall 2D convolutional auto-encoder they got was, in terms of ROC-AUC on both the JetHT dataset, and on the $t\\bar{t}$ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214e96f8-1e55-4ec8-9030-84e497d36f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749843173.904837 2160825 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13762 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "I0000 00:00:1749843173.907009 2160825 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13762 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:81:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,216</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,504</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,005</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │            \u001b[38;5;34m20\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m3,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m4,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m3,216\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m41,504\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │         \u001b[38;5;34m4,005\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,961</span> (242.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,961\u001b[0m (242.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,743</span> (241.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,743\u001b[0m (241.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">218</span> (872.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m218\u001b[0m (872.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749843177.684348 2161150 service.cc:152] XLA service 0x7fbbec004690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1749843177.684381 2161150 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1749843177.684386 2161150 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "2025-06-13 14:32:57.801518: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1749843178.293016 2161150 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-06-13 14:32:59.035724: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.16 = (f32[32,32,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,16,8,8]{3,2,1,0} %bitcast.16627, f32[32,16,3,3]{3,2,1,0} %bitcast.15093, f32[32]{0} %bitcast.16687), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_1_2/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-06-13 14:32:59.155682: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.19 = (f32[32,5,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,32,16,16]{3,2,1,0} %bitcast.15453, f32[5,32,5,5]{3,2,1,0} %bitcast.15460, f32[5]{0} %bitcast.17582), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_2_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  31/2187\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.3461 - mae: 0.6026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749843182.861238 2161150 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2186/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8894 - mae: 0.1225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:33:14.210423: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.16 = (f32[2,32,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[2,16,8,8]{3,2,1,0} %bitcast.16584, f32[32,16,3,3]{3,2,1,0} %bitcast.15081, f32[32]{0} %bitcast.16644), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_1_2/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-06-13 14:33:14.319535: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.19 = (f32[2,5,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[2,32,16,16]{3,2,1,0} %bitcast.15431, f32[5,32,5,5]{3,2,1,0} %bitcast.15438, f32[5]{0} %bitcast.17535), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_2_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.8893 - mae: 0.1225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:33:19.896959: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.16 = (f32[7,32,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[7,16,8,8]{3,2,1,0} %bitcast.866, f32[32,16,3,3]{3,2,1,0} %bitcast.873, f32[32]{0} %bitcast.875), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_1_2/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-06-13 14:33:19.993159: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.19 = (f32[7,5,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[7,32,16,16]{3,2,1,0} %bitcast.1071, f32[5,32,5,5]{3,2,1,0} %bitcast.1078, f32[5]{0} %bitcast.1080), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_2_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 1.8892 - mae: 0.1225 - val_loss: 2.5733 - val_mae: 0.0495\n",
      "Epoch 2/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.4231 - mae: 0.0522 - val_loss: 2.4020 - val_mae: 0.0281\n",
      "Epoch 3/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6775 - mae: 0.0453 - val_loss: 2.4204 - val_mae: 0.0313\n",
      "Epoch 4/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.1016 - mae: 0.0406 - val_loss: 2.4049 - val_mae: 0.0320\n",
      "Epoch 5/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.8931 - mae: 0.0388 - val_loss: 2.3777 - val_mae: 0.0289\n",
      "Epoch 6/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.7394 - mae: 0.0381 - val_loss: 2.3100 - val_mae: 0.0292\n",
      "Epoch 7/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.7435 - mae: 0.0406 - val_loss: 2.2747 - val_mae: 0.0287\n",
      "Epoch 8/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.3022 - mae: 0.0447 - val_loss: 2.3629 - val_mae: 0.0290\n",
      "Epoch 9/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.9667 - mae: 0.0448 - val_loss: 2.3593 - val_mae: 0.0317\n",
      "Epoch 10/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.5270 - mae: 0.0434 - val_loss: 2.2829 - val_mae: 0.0330\n",
      "Epoch 11/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.4997 - mae: 0.0432 - val_loss: 2.2696 - val_mae: 0.0311\n",
      "Epoch 12/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.9175 - mae: 0.0451 - val_loss: 2.2787 - val_mae: 0.0329\n",
      "Epoch 13/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6368 - mae: 0.0456 - val_loss: 2.2643 - val_mae: 0.0365\n",
      "Epoch 14/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.9804 - mae: 0.0482 - val_loss: 2.2429 - val_mae: 0.0371\n",
      "Epoch 15/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.8603 - mae: 0.0529 - val_loss: 2.2183 - val_mae: 0.0450\n",
      "Epoch 16/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6004 - mae: 0.0496 - val_loss: 2.3577 - val_mae: 0.0348\n",
      "Epoch 17/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.0310 - mae: 0.0545 - val_loss: 2.3869 - val_mae: 0.0407\n",
      "Epoch 18/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.7690 - mae: 0.0531 - val_loss: 2.2502 - val_mae: 0.0349\n",
      "Epoch 19/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.8169 - mae: 0.0509 - val_loss: 2.1615 - val_mae: 0.0355\n",
      "Epoch 20/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 2.0284 - mae: 0.0544 - val_loss: 2.1526 - val_mae: 0.0362\n",
      "Epoch 21/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.5574 - mae: 0.0509 - val_loss: 2.2428 - val_mae: 0.0426\n",
      "Epoch 22/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.1030 - mae: 0.0519 - val_loss: 1.9082 - val_mae: 0.0349\n",
      "Epoch 23/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.3839 - mae: 0.0568 - val_loss: 2.1607 - val_mae: 0.0374\n",
      "Epoch 24/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.2712 - mae: 0.0547 - val_loss: 1.9080 - val_mae: 0.0358\n",
      "Epoch 25/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.0815 - mae: 0.0541 - val_loss: 1.8735 - val_mae: 0.0398\n",
      "Epoch 26/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.7037 - mae: 0.0621 - val_loss: 1.9454 - val_mae: 0.0368\n",
      "Epoch 27/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.8344 - mae: 0.0606 - val_loss: 2.1379 - val_mae: 0.0400\n",
      "Epoch 28/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.2817 - mae: 0.0562 - val_loss: 2.1816 - val_mae: 0.0358\n",
      "Epoch 29/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6796 - mae: 0.0645 - val_loss: 2.0675 - val_mae: 0.0359\n",
      "Epoch 30/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.5445 - mae: 0.0611 - val_loss: 2.0922 - val_mae: 0.0366\n",
      "Epoch 31/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6910 - mae: 0.0602 - val_loss: 1.7676 - val_mae: 0.0421\n",
      "Epoch 32/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.2499 - mae: 0.0586 - val_loss: 2.1055 - val_mae: 0.0364\n",
      "Epoch 33/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.3192 - mae: 0.0611 - val_loss: 1.7519 - val_mae: 0.0397\n",
      "Epoch 34/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.9403 - mae: 0.0639 - val_loss: 1.9060 - val_mae: 0.0348\n",
      "Epoch 35/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.2878 - mae: 0.0594 - val_loss: 1.9006 - val_mae: 0.0421\n",
      "Epoch 36/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.5557 - mae: 0.0653 - val_loss: 1.9835 - val_mae: 0.0415\n",
      "Epoch 37/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6355 - mae: 0.0635 - val_loss: 1.8321 - val_mae: 0.0399\n",
      "Epoch 38/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.3685 - mae: 0.0634 - val_loss: 1.9999 - val_mae: 0.0360\n",
      "Epoch 39/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.6897 - mae: 0.0688 - val_loss: 2.0175 - val_mae: 0.0388\n",
      "Epoch 40/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.3901 - mae: 0.0694 - val_loss: 1.9674 - val_mae: 0.0366\n",
      "Epoch 41/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.2404 - mae: 0.0664 - val_loss: 1.8216 - val_mae: 0.0376\n",
      "Epoch 42/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1.7988 - mae: 0.0701 - val_loss: 1.7652 - val_mae: 0.0380\n",
      "Epoch 43/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1.5997 - mae: 0.0680 - val_loss: 1.8561 - val_mae: 0.0450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fbf42660880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Exercise: Make a 2D convolutional auto-encoder, and try changing some of the parameters\n",
    "#\n",
    "\n",
    "zerobias_train, zerobias_testval = train_test_split(\n",
    "    zerobias_data,\n",
    "    test_size=0.4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "zerobias_val, zerobias_test = train_test_split(\n",
    "    zerobias_testval,\n",
    "    test_size=0.2/0.4,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "conv_ae = keras.Sequential([\n",
    "    #\n",
    "    # encoder\n",
    "    #\n",
    "    keras.layers.Input(shape=zerobias_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=7,\n",
    "        padding='same',\n",
    "        activation='leaky_relu',\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    #Size is now 8 x 8, 16 filters\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "    keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        activation='leaky_relu'\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.GlobalMaxPooling2D(),\n",
    "    #\n",
    "    # Final latent space size and shape: 32 flat entries\n",
    "    #\n",
    "    #\n",
    "    # decoder\n",
    "    #\n",
    "    keras.layers.Dense(4*4*8, activation='leaky_relu'),\n",
    "    keras.layers.Reshape((4,4,8)), #new shape is 4 x 4 with 8 features\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    #keras.layers.Conv2DTranspose(16, kernel_size=2, strides=2, activation='leaky_relu'), # new shape is 8 x 8 with 16 features\n",
    "    keras.layers.Conv2DTranspose(16, kernel_size=5, strides=1, activation='leaky_relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    #keras.layers.Conv2DTranspose(32, kernel_size=2, strides=2, activation='leaky_relu'), # new shape is 16 x 16 with 32 features\n",
    "    keras.layers.Conv2DTranspose(32, kernel_size=9, strides=1, activation='leaky_relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    keras.layers.Conv2D(5, kernel_size=5, padding='same'), # final shape is 16 x 16 with 5 features\n",
    "])\n",
    "\n",
    "conv_ae.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "conv_ae.summary()\n",
    "\n",
    "conv_ae.fit(\n",
    "    x=zerobias_data,\n",
    "    y=zerobias_data,\n",
    "    validation_data=(zerobias_val, zerobias_val),\n",
    "    epochs=200,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87d6438-f0f7-40d4-bcb9-425d48b872c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1515 - mae: 0.0392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.05769681930542, 0.03884047642350197]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_ae.evaluate(zerobias_test, zerobias_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8efc9475-f6a9-453c-a49e-8aaae57ee415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m2767/2779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:41:37.723202: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.16 = (f32[27,32,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,16,8,8]{3,2,1,0} %bitcast.700, f32[32,16,3,3]{3,2,1,0} %bitcast.707, f32[32]{0} %bitcast.709), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_1_2/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-06-13 14:41:37.834974: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.19 = (f32[27,5,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,32,16,16]{3,2,1,0} %bitcast.905, f32[5,32,5,5]{3,2,1,0} %bitcast.912, f32[5]{0} %bitcast.914), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_2_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2779/2779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n",
      "\u001b[1m2019/2032\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:41:44.170357: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.16 = (f32[21,32,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[21,16,8,8]{3,2,1,0} %bitcast.700, f32[32,16,3,3]{3,2,1,0} %bitcast.707, f32[32]{0} %bitcast.709), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_1_2/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-06-13 14:41:44.275125: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.19 = (f32[21,5,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[21,32,16,16]{3,2,1,0} %bitcast.905, f32[5,32,5,5]{3,2,1,0} %bitcast.912, f32[5]{0} %bitcast.914), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_2_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2032/2032\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "\u001b[1m6384/6387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 14:42:00.761007: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.16 = (f32[8,32,8,8]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,16,8,8]{3,2,1,0} %bitcast.700, f32[32,16,3,3]{3,2,1,0} %bitcast.707, f32[32]{0} %bitcast.709), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_1_2/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-06-13 14:42:00.863849: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.19 = (f32[8,5,16,16]{3,2,1,0}, u8[0]{0}) custom-call(f32[8,32,16,16]{3,2,1,0} %bitcast.905, f32[5,32,5,5]{3,2,1,0} %bitcast.912, f32[5]{0} %bitcast.914), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_1/conv2d_2_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6387/6387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc0b1cf0310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAByY0lEQVR4nO3deVwV1f/H8de9l10EN2QTd8UdzYXULO2HqZlpWqKZuS+lfU2/5ZKWll+XNtPKEhU1S1MztUXT1MRySXPBDRQXcAdXQAFZ7j2/P65cQ0ABgQHu5/l43IfcmTMznzsI8+bMmRmdUkohhBBCCKERvdYFCCGEEMK6SRgRQgghhKYkjAghhBBCUxJGhBBCCKEpCSNCCCGE0JSEESGEEEJoSsKIEEIIITQlYUQIIYQQmrLRuoCcMJlMXLp0idKlS6PT6bQuRwghhBA5oJTi1q1beHl5oddn3/9RLMLIpUuX8PHx0boMIYQQQuTB+fPnqVSpUrbzi0UYKV26NGD+MC4uLhpXI4QQQoiciI+Px8fHx3Icz06xCCPpp2ZcXFwkjAghhBDFzMOGWMgAViGEEEJoSsKIEEIIITQlYUQIIYQQmpIwIoQQQghNSRgRQgghhKYkjAghhBBCUxJGhBBCCKEpCSNCCCGE0JSEESGEEEJoKtdh5M8//6RLly54eXmh0+lYt27dQ5cJCQnhsccew97enpo1a7JkyZI8lCqEEEKIkijXYSQhIQE/Pz/mzp2bo/aRkZF07tyZdu3aERoayptvvsngwYPZtGlTrosVQgghRMmT62fTdOrUiU6dOuW4/bx586hWrRqffvopAHXr1mXHjh189tlndOjQIbebF0IIIUQJU+APytu9ezcBAQEZpnXo0IE333wz22WSk5NJTk62vI+Pjy+o8jJJSTMREXOLq7eSSUo1kpxm5Nz1JMo42aKUQgFKgUkp+NfX6dMVyvyvUnfn3Tftbrurt5IxGHS4ONgW2mfLT+ZPUgwV07KhWJeOUsW3+uJaejEtG/jXPlcKHSZ0qAxf65QC1N2v707j7rR/vdcpE4B5OXV3Gqa761KWdekw3V32X23g7rpM6O5+neU2M9WVeV336r23rozb+Pe6VIZ6H/iZsq1f3Vfv/Z/pXhsjJvYZrrHP5hozng7Cp1q9gv8GZ6HAw0h0dDTu7u4Zprm7uxMfH09SUhKOjo6ZlpkxYwbvv/9+QZeWwbFLccz6PYJdp6+TlGos1G0LUfSYf1np7/s3+2nmX7JZTzOh02H5RazHlKFd1tPM689qmg6FXpddPf/a5r/+zVQP5nqyWl+e6sm2xvvrMR9c9PfVw33bzNju3vq4b5v/bvegaVl+n3RZfJ/I6vv0oO9dTr4nOZuWab/oinOcKvpSgfXOpVhQxoVztuY/ijed+ZXBJTWM5MWECRMYM2aM5X18fDw+Pj4Ftr24pFQ6f77D8r6Mky3eZRwpZWeDva0eO4OeiCu3aORdBnSg1+nMP6D/+hod6NCh13H3F68Ovd48w9zOPC19GYCbiSkY9DrKOtkV2GfLLw9++HPR8ZCnVN+jFAaVgo0pFVvTHXTKiB4jBpWGXhkxmFKwMSWDTodOmdAro/ngZfnaiFNqLCkGJ/N8jHf/+jCiv/tXj3k9qZRKvU6ibVnLuvXKiIE0KiRFEmfniV6l3V3ePM81JQaAJIMLeoyg1N355r9ozNtX6JSRCklRJNqWJVVvf3f95m2Uv3OOBJuyKJ3evFz6Mpn+Qsz4FxZ3DwZCFGeWiKMz/48GHUqnu+/r9Da6DNMskTqL9uj+FaF092JuhmWz2Oa9ZXUo3b2Ip3T6u/XeW4d5mu5fbe7fRsY25vXd3WYW61D/3o5OTyqw03CF9TbnuaYzn4EopWxpb6xMu2raDZ0o8DDi4eFBTExMhmkxMTG4uLhk2SsCYG9vj729fUGXZvHhxuOWr398rSWPVS6LLsdHNZEtpSDtDqQlgzHF/Eq+DWlJcCfu7vTUe/NSk8z/prdPSwZjMhjTwJRqXteVcChXHUxp5mVNRvPXplRISzG3MSZD9BEoUxlMpnvzTWnm7ZYwpdJu5mp6gdPpzS90d7/W3fdeb063mabd306XzbSs1q97yLr+PY2HbFN/X7vstvmwWoviZ9JlrCHfvk/6++p92PoftM1H2Wfmf+9WIv4l2ZjM2pNrCT4aTHRCNADlHMrRv35/An0DcbJ10rS+Ag8jLVu2ZMOGDRmmbd68mZYtWxb0pnNsx8lrALzdwZemVcppXE0hU8p80E+5bX4l34KURPNBPSXBPC32LKADZTIf5Et7QmoiXDxgPuCnJprbpSbB1ePmdrZO5ukF4cI/OWsXey5367V1Ar0t6A1gsIXbMeDgCk7lQW8DOoN5nk5/973OHI68m979RZg+P73N3X+NqZBwFdx8zdP0tveWj790d/rd7eptzK+UBHMNDq731vPv7ae/9AZzwHMqBwY78zJ6G/P6lAnsnO5u6+7yhXGQE0IUGUlpSfwY8SOLjy7mStIVANwc3RjQYAAv1n4RR5usOwUKW67DyO3btzl16pTlfWRkJKGhoZQrV47KlSszYcIELl68yNKlSwEYPnw4X375JWPHjmXgwIH88ccfrFq1ivXr1+ffp3gEJpPi3A3zQfPJWm4aV5MHJhOk3II78ZB0w/yX/504SLoJif96n3zL/Eq8Dhf2Qpkqd6fFm3sM8urKsaynZxVEDHbmAzMK3OqArePdA+jdg6jBHmwd7k2zcQAbe/PB1GBrPsDq9ObPWabKvYN3+gHYYHtvGYM9KCPYu2Q8QOvvHtBtne5uw978rxxEhRAlSGJqIqtOrGLxscXcuHMDAHcndwY1HET3Wt2xNxTe2YecyHUY2bdvH+3atbO8Tx/b0a9fP5YsWcLly5c5d+7eX6TVqlVj/fr1jB49mjlz5lCpUiUWLlxYZC7rvXr73lU7dTxLa1gJd3soEszhIf4SxJ2Hm1Fw+4r5r/SbUVCqwt1QceNe+MiL2LOZp9k6mV+J18DVB0q5gX1psHOGO7HgXNHcK3LrMrg3MIeJO/FQvqb5a3tnsHE0H/idyoFdqbvhwMEcCOSAL4QQBep2ym1WnFjB0mNLuZlsPlXrVcqLwY0G07VGV+wMRXOMYq7DSNu2bR94mV5Wd1dt27YtBw8ezO2mCsXt5Hu9ArYGfcFuLPk2XAmDW9HmA3rsubuvs3D50KOt22BvDg6OZcGxDDiWu3eKwb60+Wu7UuDgAralzD0Czu4Z5+kN+fIxhRBCFK74lHiWhS/ju7DviE+JB8CntA9DGg7huRrPYasv2reRKJJX0xSm67dTAPMVNI8sLQXiL8Dlw+aAEXvubvCIhhunzadOcsLWCRzKmHsjqj1pPo1Qrpr5FIVjWShfy9zz4FDG3FNi52w+vSGEEMKqxN6J5dvwb1kevpzbqbcBqOpSlaGNhtKpWids9MXjMF88qixAaSbzZYyxiam5XzjpJpzdBWdCYO/8u2MiUh68jIMrpN6B2h3Mp0LK+IBrJShbDVy87o5xsPpvixBCiAe4cecG3xz7hhXHV5CYZh6jV7NMTYY2GsozVZ7BUMx6uq3+qJeSZg4j9TxdcrZAwnU4sR7CfoYz2zIO/kwPIh6NzFdIuPqYx1iUdjdfjlqmsjlsyNgJIYQQeXAt6RpLji5hVcQqktKSAPAt68swv2H8X+X/Q68r4OEGBcTqw0j6aRo7mwd8A5UyX9K6LxgOfGu+SiNd2WpQpTVUa3M3hNTh7t3OhBBCiHwRkxDD4mOLWR2xmmSj+cKL+uXrM6zRMNr6tC3298ay+jCSfpomOu5O5pmx5+GfBbAnyHzfjXTla0HDF6Hu8+Cuza1zhRBClHyXbl9i0dFFrDm5hlSTeThBI7dGDG80nCe8nyj2ISSd1YeRuzdzx7vsv278kpoEa4ZC+M8ZG9fqAC2GQq2MD/4TQggh8tP5W+cJPhLMT6d+Ik2ZhwM0dW/KsEbDeNzz8RITQtJZfRhJM5kvUy5X6u6110mxsDAArp80v3f1gacnQd0u5stfhRBCiAISFRfFgiMLWH9mPca7QwL8Pf0Z1mgYzT2aa1xdwZEwcvc0ja3hbsr8rrs5iOgM0O4daPNfGXAqhBCiQJ2OPc38w/PZGLURkzIfl1p7t2Z4o+E0rthY2+IKgdWHkfSbntno9RB3ES7uN8/oMA0ef03DyoQQQpR0J26cYP7h+Ww+uxmFuae+baW2DG00lIZuDTWurvBYfRhJH7ialpYGv0+8N6PFUI0qEkIIUdKFXQ8j6FAQf5z/wzItoHIAQxsNpW75uhpWpg2rDyPpY0WaX/8JYteaJ3acKbdGF0IIke8OXz1M0OEg/rzwJ2C+iKJD1Q4MaTSE2mVra1yddqw+jNwdv8rzSXeDSJ3n5PSMEEKIfHUg5gBBh4PYdWkXAHqdnk7VOjG04VCql6mucXXakzByN42UT75gntC4j4bVCCGEKCmUUuyL2ce8Q/PYG70XAIPOwHPVn2NIoyFUcamicYVFh4QRpfDm6r0J1Z7UrhghhBDFnlKK3Zd3E3QoiANXDgBgo7eha42uDGo4CJ/SPhpXWPRIGFHQynDs3gR7Z+2KEUIIUWwppfjr4l8EHQri8LXDANjqbeleqzuDGgzC09lT4wqLLgkjSlFPd9b8xquJtsUIIYQodpRSbDu/jaDDQYRdDwPA3mDPS7Vfon/9/riXcte4wqJPwohJ8bzBPKCIJq9oW4wQQohiw6RMbDm7haDDQUTcjADA0caRQN9A+tXvRwXHChpXWHxYfRgxKoWRu5fxulTSthghhBBFntFkZFPUJuYfns/puNMAONk48XLdl+lbry/lHMppXGHxY/VhxCYtiYq6WPMb78c0rUUIIUTRlWZKY0PkBhYcXkBUfBQApW1L06deH16p+wqu9q7aFliMWX0YuXQh6t6bUm6a1SGEEKJoSjWm8suZX1hweAEXbptvA+Fi58Kr9V6ld93euNi5aFxh8Wf1YaRu6SS4DrcNrjjLA/GEEELclWJMYd2pdQQfCeZSwiUAytqXpV/9fvSq04tStvIk9/xi9WHExpQKgLMxTuNKhBBCFAV30u7w48kfWXR0EVcSrwBQ3qE8AxoM4KXaL+Fk66RxhSWP1YeRsinmtHvFpQEVNa5FCCGEdhJTE/kh4geWHFvCtaRrAFR0rMjAhgPpUasHDjYOGldYcll9GDGiB8Al8ZzGlQghhNBCQmoCK0+s5Jtj33Djzg0APEt5MqjBILrV6oa9wV7jCks+qw8jtqYkAKLLNqeqtqUIIYQoRLdSbvH98e9ZGraUuGTzqXpvZ2+GNBzC8zWex9Zgq3GF1sPqw4h7chQASm/QthAhhBCFIi45jmXhy/gu/DtupdwCoIpLFYY0HMKz1Z/FVi8hpLBZfRi5bTBfF+6QclPjSoQQQhSkm3du8m3Ytyw/vpyE1AQAqrtWZ2ijoXSs2hGD/FGqGasPIzbKfDXNTZc6yCOMhBCi5LmWdI2lx5ay4sQKktLMp+Zrl63N0EZDaV+lPXqdXuMKhdWHkbKpMQCY9HYaVyKEECI/XUm8wuKji1kdsZo7xjsA1C1Xl2F+w2jn005CSBFi9WHE1pQMgI1K1rgSIYQQ+SE6IZrgI8GsObmGFFMKAI0qNGKY3zDaeLdBJze4LHKsPozc0TkCYDQ4alyJEEKIR3Hx9kUWHlnIulPrSDOlAdCkYhOGNxpOS6+WEkKKMKsPIzbKnJrvOMgtz4QQojg6F3+OBUcW8OvpX0lT5hDS3KM5wxsNp7lHcwkhxYDVh5Fadw4DYDLImBEhhChOzsSdYcHhBWyI3IBJmQBo6dmSYX7DaOreVOPqRG5YfRiJVF40Jg5dapLWpQghhMiBkzdPMv/wfDZFbUKhAGjj3YZhfsPwc/PTuDqRF1YfRkrbpEEq3HKspHUpQgghHuD4jeMEHQpiy7ktlmntfNoxzG8Y9cvX17Ay8aisPozUSD0JgFMpeRS0EEIURUevHSXoUBAhF0IA0KEjoEoAwxoNw7ecr7bFiXxh9WHkpr4sZU03UXL7XyGEKFJCr4Qy7/A8dl7cCZhDSMdqHRnacCg1y9bUuDqRn6w7jChFWZP5NvApjnI1jRBCFAX7ovcx7/A89lzeA4BBZ6Bz9c4MbjiYaq7VNK5OFAQrDyMmy5dGWzlNI4QQWlFKsSd6D0GHgtgXsw8AG50Nz9d8nsENBuPj4qNxhaIgWXcYMRnvfW1w0K4OIYSwUkopdl7aSdChIEKvhgJgo7ehe83uDGw4EG9nb20LFIXCysNI2r2v5RkFQghRaJRSbL+wnaBDQRy9fhQAO70dL9Z+kQENBuBRykPjCkVhsu4wou71jOhsrHtXCCFEYTApE3+c+4P5h+cTfiMcAAeDAz19e9K/fn/cnNw0rlBowbqPwBl6Rgza1SGEECWc0WRk89nNBB0O4lTsKQAcbRzpVacX/er1o7xjeY0rFFqy8jBybwAregkjQgiR39JMaWyM2sj8w/OJjIsEwNnWmd51etO3Xl/KOpTVuEJRFFh3GDEmA2BSOnQSRoQQIt+kmlJZf2Y9Cw4v4NytcwCUtitN37p9ebnuy7jau2pcoShKrDuMpN0BQK9T6OWhjkII8chSjan8dPonFh5ZyMXbFwEoY1+GV+u9Su86vXG2c9a4QlEUWXcYUeYHLN1WDuiQNCKEEHmVbExm7cm1BB8NJjohGoByDuXoX78/gb6BONk6aVyhKMqsPIyYx4ykYZCeESGEyIOktCR+jPiRxUcXcyXpCgBujm4MaDCAF2u/iKONo8YViuLAusPI3ZuemdAhHSNCCJFziamJrDqxisXHFnPjzg0A3J3cGdRwEN1rdcfeYK9xhaI4se4wcrdnxIgevU7SiBBCPMztlNusOLGCpceWcjPZ/Gwvb2dvBjUcRNcaXbEz2GlcoSiOrDyMmHtGFHpS0kwPaSyEENYrPiWeZeHL+C7sO+JT4gGoXLoygxsO5rkaz2ErTz4Xj8DKw8i9nhFXR/lBEkKI+8XeieXb8G9ZHr6c26m3AajmWo0hDYfQqVonbPTWfRgR+cO6/xfdDSMmdBhkBKsQQljcuHODb459w4rjK0hMSwSgZpmaDGs0jPZV2mOQezOJfGTdYeTuHVgVOmTIiBBCwLWkayw5uoRVEatISksCwLesL8P9hvN05afRy0NFRQGw7jByd8yIUckAViGEdYtOiGbx0cX8ePJHku/enbp++foM9xvOU5WeQie/I0UBsu4wkmQeCe6oS+GO/KAJIazQpduXCD4SzNpTa0k1pQLg5+bHcL/htPZqLSFEFArrDiM25uvgK+piiZOfNyGEFTkff56FRxfy86mfSVPmJ5g3dW/KcL/h+Hv4SwgRhSpPJ//mzp1L1apVcXBwwN/fn7179z6w/ezZs/H19cXR0REfHx9Gjx7NnTt38lRwQQg3+cgPnhDCKkTFRTFxx0S6rOvCmpNrSFNp+Hv6s6jDIpZ0XMLjno/L70NR6HLdM7Jy5UrGjBnDvHnz8Pf3Z/bs2XTo0IETJ05QsWLFTO2XL1/O+PHjWbRoEa1atSIiIoL+/fuj0+mYNWtWvnyI/CAX0wghSrLTsacJOhzEpqhNmO5eSdjauzXDGw2nccXG2hYnrF6uw8isWbMYMmQIAwYMAGDevHmsX7+eRYsWMX78+Eztd+3aRevWrXn55ZcBqFq1Kr1792bPnj2PWHo+uPugPEAGsAohSqQTN04w//B8Np/djML8O69tpbYMbTSUhm4NNa5OCLNchZGUlBT279/PhAkTLNP0ej0BAQHs3r07y2VatWrFd999x969e2nRogVnzpxhw4YN9O3bN9vtJCcnk5ycbHkfHx+fmzLzQCdhRAhRooRdDyPoUBB/nP/DMi2gcgBDGw2lbvm6GlYmRGa5CiPXrl3DaDTi7u6eYbq7uzvHjx/PcpmXX36Za9eu8cQTT6CUIi0tjeHDh/POO+9ku50ZM2bw/vvv56a0RyZZRAhREhy+epigw0H8eeFPAHTo6FC1A0MaDaF22doaVydE1gr87jUhISFMnz6dr776igMHDrBmzRrWr1/P1KlTs11mwoQJxMXFWV7nz58v6DLRy6ARIUQxdiDmAMM2D6PPhj78eeFP9Do9nat3Zl3XdXz81McSRESRlquekQoVKmAwGIiJickwPSYmBg8PjyyXeffdd+nbty+DBw8GoGHDhiQkJDB06FAmTpyIXp85D9nb22NvX/CPn1ZKkR5BJIsIIYobpRT7YvYx79A89kabr2o06Aw8V/05hjQaQhWXKhpXKETO5CqM2NnZ0bRpU7Zu3Uq3bt0AMJlMbN26lZEjR2a5TGJiYqbAYTCYn2mg/jWAVEsKHfY28pwFIUTxoJRi9+XdBB0K4sCVAwDY6G3oWqMrgxoOwqe0j8YVCpE7ub6aZsyYMfTr149mzZrRokULZs+eTUJCguXqmldffRVvb29mzJgBQJcuXZg1axZNmjTB39+fU6dO8e6779KlSxdLKNGKUiAdIkKI4kIpxV8X/yLoUBCHrx0GwFZvS/da3RnUYBCezp4aVyhE3uQ6jAQGBnL16lXee+89oqOjady4MRs3brQMaj137lyGnpBJkyah0+mYNGkSFy9exM3NjS5dujBt2rT8+xT5QEKJEKKoUkqx7fw2gg4HEXY9DAB7gz0v1X6J/vX7417K/SFrEKJo06micq7kAeLj43F1dSUuLg4XF5d8W6/x5FYMy7oTZqqC57h9lC1ll2/rFkKIR2VSJrac3ULQ4SAibkYA4GjjSKBvIP3q96OCYwWNKxTiwXJ6/LbuZ9PcpZBLe4UQRYfRZGRT1CbmH57P6bjTADjZOPFy3ZfpW68v5RzKaVyhEPlLwogQQhQRaaY0NkRuYMHhBUTFRwFQ2rY0fer14ZW6r+Bq76ptgUIUEOsOI3efzyCEEFpKNabyy5lfWHB4ARduXwDAxc6FV+u9Su+6vXGxy7/T00IURdYdRu5S6NDJEFYhRCFLMaaw7tQ6go8EcynhEgBl7cvSr34/etXpRSnbUhpXKEThsOowUuRH7gohSqQ7aXf48eSPLDq6iCuJVwAo71CeAQ0G8FLtl3CyddK4QiEKl1WHkQykY0QIUcASUxP5IeIHlhxbwrWkawBUdKrIwAYD6VGrBw42DhpXKIQ2rDqMFIOrmoUQJUBCagIrT6zkm2PfcOPODQA8S3kyuOFgutXshp1BbisgrJtVh5F0cmmvEKIg3Eq5xffHv2dp2FLikuMAqORciSGNhtClehdsDbYaVyhE0WDVYUTJqBEhRAGIS47ju/DvWBa+jFsptwCo4lKFoY2G8my1Z7HRW/WvXiEykZ+Iu6RjRAjxqG7eucnSsKV8f/x7ElITAKjuWp1hjYbRoWoHDHp5IKcQWbHuMGKSnhEhxKO7lnSNb459w8oTK0lKSwKgdtnaDG00lPZV2qPX6R+yBiGsm3WHkbsUOnQyaEQIkUtXEq+w+OhiVkes5o7xDgB1y9VlmN8w2vm0kxAiRA5JGBFCiFyKTogm+Egwa06uIcWUAkCjCo0Y5jeMNt5t5I8bIXJJwogQQuTQhVsXCD4azLpT60gzpQHQpGIThjcaTkuvlhJChMgjCSN3ya8QIUR2zsWfY8GRBfxy+heMyghAc4/mDG80nOYezSWECPGIrDqMpN/zTEkUEUJk4UzcGRYcXsCGyA2Y7j5Ys6VnS4b5DaOpe1ONqxOi5LDqMPJv8oeNECLdyZsnmX94PpuiNlnuR9TGuw3D/Ibh5+ancXVClDxWHUbkpmdCiH87fuM4QYeC2HJui2VaO592DPMbRv3y9TWsTIiSzarDyL+f26uTUzVCWK2j144SdCiIkAshgPn3QUCVAIY1GoZvOV9tixPCClh3GFEZ/hFCWJnQK6HMOzyPnRd3AuYQ0rFaR4Y2HErNsjU1rk4I62HdYeRfZMyIENZjX/Q+5h2ex57LewAw6Ax0rt6ZwQ0HU821msbVCWF9rDqMSI+IENZDKcWe6D0EHQpiX8w+AGx0Njxf83kGNxiMj4uPxhUKYb2sOoxYru0VQpRYSil2XtpJ0KEgQq+GAmCjt6F7ze4MbDgQb2dvbQsUQlh5GLGQczRClDRKKbZf2E7QoSCOXj8KgJ3ejhdrv8iABgPwKOWhcYVCiHRWHUbk0l4hSh6TMvHHuT+Yf3g+4TfCAXAwONDTtyf96/fHzclN4wqFEPez6jDy79M0MoBViOLNaDKy+exmgg4HcSr2FACONo70qtOLfvX6Ud6xvMYVCiGyY91h5C65HbwQxVeaKY2NURuZf3g+kXGRADjbOtO7Tm/61utLWYeyGlcohHgYCSN3yU3PhCheUk2prD+zngWHF3Du1jkAStuVpm+9vvSp2wcXOxeNKxRC5JRVhxEZMSJE8ZNqTOWn0z+x8MhCLt6+CEAZ+zL0q9+PXr69cLZz1rhCIURuWXUYkTEjQhQfycZk1p5cS/DRYKITogEo51COAfUH0NO3J062ThpXKITIKysPIxn+EUIUQUlpSfwY8SOLjy7mStIVANwc3RjQYAAv1n4RRxtHjSsUQjwq6w4j/yIdI0IULYmpiaw6sYrFxxZz484NANyd3BnUcBDda3XH3mCvcYVCiPxi1WFEekSEKHpup9xmxYkVfHPsG2KTYwHwdvZmUMNBdK3RFTuDnbYFCiHynVWHkX/HEZ0MGhFCU/Ep8SwLX8Z3Yd8RnxIPQOXSlRnccDDP1XgOW72txhUKIQqKlYcRM7nPiBDaib0Ty7fh37I8fDm3U28DUM21GkMaDqFTtU7Y6OXXlBAlnXX/lMt5GiE0cz3pOkvDlrLi+AoS0xIBqFmmJsMaDaN9lfYY9AaNKxRCFBbrDiP/In0jQhSOq4lXWXJsCatOrOKO8Q4AvmV9Ge43nKcrP41ep9e4QiFEYbPqMCIPyhOi8EQnRLP46GJ+PPkjycZkAOqXr89wv+E8VekpGbclhBWz6jCSTqGTm54JUUAu3b5E8JFg1p5aS6opFQA/Nz+G+w2ntVdrCSFCCOsOI0pJz4gQBeV8/HkWHl3Iz6d+Jk2lAdDUvSnD/Ybj7+EvIUQIYWHVYeTf5BejEPkjKi6KBUcWsP7MeozKCIC/pz/DGg2juUdzjasTQhRFVh5GpGdEiPxyOvY0QYeD2BS1CZMyAdDauzXDGw2nccXG2hYnhCjSrDyMmEkkESLvTtw4wfzD89l8drNlUHjbSm0Z2mgoDd0aalydEKI4sOowIkNGhMi7sOthBB0K4o/zf1imBVQOYGijodQtX1fDyoQQxY1VhxEhRO4dvnqYoMNB/HnhTwB06OhQtQNDGg2hdtnaGlcnhCiOrDyMSNeIEDl1IOYAQYeD2HVpFwB6nZ5O1ToxtOFQqpeprnF1QojizMrDSDq5kkaIrCil2Bezj3mH5rE3ei8ABp2B56o/x5BGQ6jiUkXjCoUQJYGEESFEJkopdl/eTdChIA5cOQCAjd6GrjW6MqjhIHxK+2hcoRCiJLHuMGKS0zRC/JtSir8u/kXQoSAOXzsMgK3elu61ujOowSA8nT01rlAIURJZdxi5S8lpGmHllFJsO7+NoMNBhF0PA8DeYM9LtV9iQIMBVHSqqHGFQoiSzKrDSIrRfGMm6R8R1sqkTGw5u4Wgw0FE3IwAwNHGkUDfQPrV70cFxwoaVyiEsAZWHUaEsFZGk5FNUZuYf3g+p+NOA1DKthS96/Smb72+lHMop3GFQghrYuVhxNwnYpCzNMJKpJnS2BC5gQWHFxAVHwVAadvS9KnXh1fqvoKrvau2BQohrJKVhxEzGTMiSrpUYyq/nPmFBYcXcOH2BQBc7V3pW7cvL9d9mdJ2pTWuUAhhzSSMCFHC/XrmV2bvn01MYgwA5RzK8Wq9V+lVpxelbEtpXJ0QQlh5GJFn04iS7kDMASb8NQEw37b9v83+y0u1X8LJ1knjyoQQ4h59XhaaO3cuVatWxcHBAX9/f/bu3fvA9rGxsYwYMQJPT0/s7e2pXbs2GzZsyFPB+cucRuQkjSiJIm5GMHLrSAAaVmjI3y//Tb/6/SSICCGKnFz3jKxcuZIxY8Ywb948/P39mT17Nh06dODEiRNUrJj5XgQpKSm0b9+eihUrsnr1ary9vTl79ixlypTJj/ofibL8K3FElCxbz25lzPYxmJQJHTre8X9HQogQosjKdRiZNWsWQ4YMYcCAAQDMmzeP9evXs2jRIsaPH5+p/aJFi7hx4wa7du3C1tYWgKpVqz5a1flNsogoQRYcXsDnBz8HwNvZm6WdlspNy4QQRVquTtOkpKSwf/9+AgIC7q1ArycgIIDdu3dnuczPP/9My5YtGTFiBO7u7jRo0IDp06djNBqz3U5ycjLx8fEZXgVCBo2IEubr0K8tQaRHrR6seX6NBBEhRJGXqzBy7do1jEYj7u7uGaa7u7sTHR2d5TJnzpxh9erVGI1GNmzYwLvvvsunn37K//73v2y3M2PGDFxdXS0vH58CeiiXhBFRQiQbkxm8aTBfHfoKgBdrv8iUVlPk1IwQoljI0wDW3DCZTFSsWJH58+fTtGlTAgMDmThxIvPmzct2mQkTJhAXF2d5nT9/vkBrlDEjojhLSkti8KbB7IneA8Brfq8xueVkjasSQoicy9WYkQoVKmAwGIiJickwPSYmBg8PjyyX8fT0xNbWFoPBYJlWt25doqOjSUlJwc7OLtMy9vb22Nvb56a0PJF+EVGcKaVYd2odXx78kitJVwAY13wcr9R7RePKhBAid3LVM2JnZ0fTpk3ZunWrZZrJZGLr1q20bNkyy2Vat27NqVOnMJlMlmkRERF4enpmGUS0IP0ioriJiovi9a2v896u97iSdIXyDuWZ3W62BBEhRLGU69M0Y8aMYcGCBXzzzTeEh4fz2muvkZCQYLm65tVXX2XChAmW9q+99ho3btxg1KhRREREsH79eqZPn86IESPy71PklYwZEcXQ8vDl9Pi5Bzsu7gCgX71+/PzCz/xf5f/TuDIhhMibXF/aGxgYyNWrV3nvvfeIjo6mcePGbNy40TKo9dy5c+j19zKOj48PmzZtYvTo0TRq1Ahvb29GjRrFuHHj8u9TPCKJJKI4uJ1ym4/3fcyak2sA8C3ry9TWU6lbvq7GlQkhxKPRKVX0uwfi4+NxdXUlLi4OFxeXfFtv9M5leGx+nb3Up8WUXfm2XiHyW1RcFG/88YblSbu96/RmXPNxGPSGBy8ohBAayunx26qfTSN9IqI4WB2xmpl7Z5JsTMbFzoVJj0+iY9WO6HQy2kkIUTJYdRhJ7xOSS3tFUaSUYuKOifxy5hcA6pSrwxdPf4FHqayvXBNCiOLKqsNIOokioqhRSjH176mWIBLoG8g7/u+g1xX4rYGEEKLQSRgRoohRSjFp5yR+Pv0zAG8+9iaDGg7SuCohhCg4Vh5GZMyIKFquJV1j0s5J7Ly4E4C3mr1Fv/r9NK5KCCEKlpWHETMZMyKKgj2X9/DW9reITY4FYGzzsfSt11fbooQQohBYdRgp+hc1C2sQlxzHp/s+Ze2ptQB4O3szueVkWnplfVdjIYQoaaw6jFhIx4jQyOnY0wzcNJAbd24A8Gy1Z3n38XdxtnPWuDIhhCg8Vh5GTA9vIkQBORN7hsBfA0k2JlPBsQLvt3qfJys9qXVZQghR6Kw8jKSTrhFRuCLjInllwyskG5Nxc3Rjeeflcv8QIYTVkpsWCFHIzt86T/+N/bmVeguPUh4se3aZBBEhhFWz6jAiA1hFYfsn+h96/tKTG3duYNAZmP7EdDydPbUuSwghNGXVYUSIwqKUYtWJVQzdPJTbqbfxKe3Dt52+pblHc61LE0IIzVn1mBF5No0oDHHJcby1/S3+vvw3AI9VfIzPn/4cV3tXjSsTQoiiwarDiBAFLSYhhn4b+3Hx9kUMOgMDGwxkZJOR8owZIYT4F6sOI4bkG1qXIEqwa0nXGPz7YC7evoiLnQufP/05Td2bal2WEEIUOVYdRtAZAPBRlzUuRJQ015Ou03dDXy7cvoCjjSOLOizCt5yv1mUJIUSRZNV9xSaDHQDROjeNKxElyambp+j+c3dLEFnccbEEESGEeACrDiPpD+1NwEnbOkSJsTx8OT1+6cGNOzcoZVuK4GeCqV++vtZlCSFEkWbdp2mEyCdKKeYcmEPw0WAAGlVoxKdtP5WbmQkhRA5IGBHiERlNRib8NYHfon4DoHed3oxvMV6umBFCiByy8jBy9zyN3GZE5FGyMZkRW0ew5/IeAN5o8gZDGg5Bp5P/VEIIkVNWHUbkpmfiUYRdD+Ot7W9x/tZ5AN5v9T7da3XXuCohhCh+rDqMpJMoInIr5HwIb/zxBgBl7csy7YlptKnURtuihBCimJIwIkQu7Y/Zz6htowDwLetLUPsgyjuW17gqIYQoviSMCJELYdfDGPz7YEzKRK2ytVjWeRn2BnutyxJCiGLNyof7K60LEMXIxqiN9N/YnzRTGjXL1CQoIEiCiBBC5AOr7hmRAawiJxJSE5j29zR+OfMLADXL1GThMwvl1IwQQuQTqw4jQjzMpduXGLZ5GFHxUQC8UPMFxrcYj5Ot3LVXCCHyi4QRIbJx7Noxhmwewq2UW5S2Lc37rd+nfZX2WpclhBAljoQR5NJekdkPET8w/e/ppKk0PEp5ML/9fKq5VtO6LCGEKJGsOowoZdK6BFHEGE1GJu6cyPoz6wF4wvsJpj0xjXIO5TSuTAghSi6rDiPplHSNCMwPuxsdMppt57cBMKThEEY2GSnPmBFCiAImYUQIID4lnsk7J1uCyPQnptOlRheNqxJCCOsgYURYveiEaAZuGmh5xswk/0kSRIQQohBJGAF0MoTVakUnRNP95+7cSrmFo40js9rO4gnvJ7QuSwghrIpVhxGl5A6s1ux8/Hn6/taXWym3sNHbsPzZ5dQsW1PrsoQQwurIyDzkpvDW6Ni1YwT+Gsj1O9cp51COFZ1XSBARQgiNSBgRVmfViVX0Wt+LW6m3qORciW87fYtvOV+tyxJCCKtl1adphPX549wfTP17KgD1y9fny//7kgqOFTSuSgghrJtVh5GUNLnpmTVZFr6MmXtnAvBYxccI7hCMjd6qfwSEEKJIsOrTNOnX0CSmGDWtQxS8X07/YgkibX3asuCZBRJEhBCiiLDq38a6u2mklL2ttoWIArXk6BI+3f8pYA4in7f7HJ1OLucWQoiiwqp7RtLJcankWh2x2hJEOlfvzKy2sySICCFEEWPVPSOi5EpMTWTanmn8fPpnAJ6v8Tz/a/0/CSJCCFEESRgRJY5SilHbRvH35b8B6FGrB5MenyRBRAghiiirDiNyA9aSx6RMjPtznCWIfPrUpzxT9RmNqxJCCPEgVj5mRNJISaKUYurfU9kYtRGAkY1HShARQohiwKp7Ru6R7vviTinF2D/HWoLI+Bbj6VO3j8ZVCSGEyAkr7xkRJUGyMZnxf423BJHRTUdLEBFCiGJEekZEsXY18Sojto4g/EY4AAMaDGBgg4EaVyWEECI3rDyMyJiR4izZmMzTPzwNgL3BnimtpvBc9ec0rkoIIURuWXcYkctpii2lFOP+HGd5/3XA1zT3aK5hRUIIIfJKxowASgawFismZWLyrslsPbcVgGlPTJMgIoQQxZiEEVHszDs0j7Wn1gLQu05vnq/xvMYVCSGEeBTWfZpGFDsbIzfy9aGvAehfvz//bfZfjSsSQgjxqPLUMzJ37lyqVq2Kg4MD/v7+7N27N0fLrVixAp1OR7du3fKy2XwnI0aKl2Xhy3j7z7cBeKrSU4xpOkbjioQQQuSHXIeRlStXMmbMGCZPnsyBAwfw8/OjQ4cOXLly5YHLRUVF8dZbb9GmTZs8F5vfdHcHsMqIkaLtdsptRm8bzcy9MwGoV74enzz1iTxrRgghSohch5FZs2YxZMgQBgwYQL169Zg3bx5OTk4sWrQo22WMRiN9+vTh/fffp3r16o9UcEGQAaxF16Xbl+j7W1+2nNsCwEu1X2Jxh8U42DhoXJkQQoj8kqswkpKSwv79+wkICLi3Ar2egIAAdu/ene1yH3zwARUrVmTQoEE52k5ycjLx8fEZXsL6bD67mc5rOnMq9hQOBgcWdVjEey3fw8nWSevShBBC5KNchZFr165hNBpxd3fPMN3d3Z3o6Ogsl9mxYwfBwcEsWLAgx9uZMWMGrq6ulpePj09uyhQlwIYzGxgTMoY0lUYl50p89+x3cvmuEEKUUAV6ae+tW7fo27cvCxYsoEKFCjlebsKECcTFxVle58+fL5D6ZABr0fRb5G+8s+MdAOqXr8/PL/yMbzlfjasSQghRUHJ1aW+FChUwGAzExMRkmB4TE4OHh0em9qdPnyYqKoouXbpYpplMJvOGbWw4ceIENWrUyLScvb099vb2uSnt0ciQkSLjQMwBxv81HpMy8YT3E8xpNwdbva3WZQkhhChAueoZsbOzo2nTpmzdutUyzWQysXXrVlq2bJmpfZ06dThy5AihoaGW1/PPP0+7du0IDQ0tMqdfpIekaLidcpvRIaMxKRMtPVvy5dNfYmew07osIYQQBSzXNz0bM2YM/fr1o1mzZrRo0YLZs2eTkJDAgAEDAHj11Vfx9vZmxowZODg40KBBgwzLlylTBiDTdGHdko3JjA4ZzY07N3C2deaD1h9g0Bu0LksIIUQhyHUYCQwM5OrVq7z33ntER0fTuHFjNm7caBnUeu7cOfR6ucu8yLmj144yOmQ00QnR6HV6PnzyQzxKZT7tJ4QQomTK0+3gR44cyciRI7OcFxIS8sBllyxZkpdNFhA5QaO1sOthDNw0kKS0JFzsXHi35bs8WelJrcsSQghRiKz72TRyB1ZNrTi+gul7pqNQeDt7892z31HBMedXXQkhhCgZ5HwKcgdWLRy7dswSRPzc/Fj27DIJIkIIYaUkjIhCt/38dvpv7I9C4VHKg4XPLKS8Y3mtyxJCCKERCSOiUO2P2c/4v8Zzx3iH6q7V+b7z9/KcGSGEsHJWPWZEhq8WrhXHVzBtzzQAKjpV5Ntnv8XFzkXjqoQQQmjNqntGdBJHCs1XoV9ZgshjFR/jhy4/SBARQggBWHnPyD0ygLUgbT27la8PfQ1AO592zGo7Cxu9/NcTQghhJkcEUWASUxP54uAXfBf+HQBPVnqSz5/+XOOqhBBCFDUSRkSBSExNZMjmIRy+ehiAxz0f59OnPtW4KiGEEEWRdYcRGTJSIEzKxLg/x3H46mFs9DZMbT2VztU6o9PJ6TAhhBCZWXcYSU8jcozMV+/tfI+QCyEAzGk3R27vLoQQ4oGs+mqadHIH1vyz8vhKfjr9EwDjW4yXICKEEOKhJIyIfLPqxCrL5bu96/SmT90+GlckhBCiOLDy0zQiv2w/v52pf08FoFPVToxtPlbjioQQQhQXEkbEIzt09RAj/xgJQFP3psxoMwOD3qBxVUIIIYoLKz9NI5fTPKrbKbcZ9+c4ADxLefLV/30lQUQIIUSuWHkYEY9CKcWQ34dw8fZFStmWYuEzC3GyddK6LCGEEMWMhBGRZ3ND53L0+lEAPn3qUyq7VNa4IiGEEMWRhBGRJ0uPLSXocBAAgb6BtPZurXFFQgghiisZwCpybfb+2QQfDQbguerPMdF/osYVCSGEKM6sumdEyfjVXFsevtwSRHrU6sG0J6bJbd6FEEI8EivvGUlPI3IwzYl1p9YxY+8MAAIqBzCl1RRtCxJCCFEiWHXPSDrpIHm4sOthTNk1BYDHKj7GR09+pG1BQgghSgwr7xkROREZF8mAjQMwKiOPVXyMhR0WYqu31bosIYQQJYT0jIgHupp4lUGbBpGYloi3szez2s6SICKEECJfWXcYkfMzDxSfEk/f3/pyNekqzrbOLOm4hPKO5bUuSwghRAlj3WHkbhqR4auZmZSJARsHcPH2RRwMDgR3CMajlIfWZQkhhCiBrDyMmCmJIxmYlIl3drxDxM0IAKa3mU698vU0rkoIIURJJQNYRQapxlTe2v4Wf5z/A4C3mr1F+yrtNa5KCCFESSZhRFjEp8TzVshb7L68G71Oz5SWU3ih1gtalyWEEKKEkzAiAPOpmW7runE16So2ehtmtJlBx6odtS5LCCGEFbDuMCI3YAUg1ZTK6G2juZp0FYDP231Om0ptNK5KCCGEtZABrIC1p5F5h+ax/cJ2AEY9NkqCiBBCiEJl3T0jgr2X97LwyEIARjcdzcAGAzWuSAghhLWRnhEr9se5Pxj0+yBMyoS/hz/96/fXuiQhhBBWyKrDiLLiW7AqpZi0YxIA5RzK8fFTH6PXWfV/ByGEEBqx6qOPzorDyCf7PuFW6i0AZraZSVmHshpXJIQQwlpZdRhJZ213YN11cRffhX8HwGt+r9HSq6XGFQkhhLBmEkaszMbIjQzbMgyTMtGwQkOGNRqmdUlCCCGsnIQRK3Ls+jHe/vNtANyd3FnUYREGvUHjqoQQQlg7qw4j1jRi5EzsGfpu6Gt5/22nb3GwcdCwIiGEEMLMqsMIyhxHSvqIkcu3L9NvYz9STal4lPJge+B2PJ09tS5LCCGEAKw9jNxVkgewxt6JZfiW4cQmx+Lt7M2iDoso51BO67KEEEIIC7kDawkWlxxHj597cCXpCvYGez5/+nN8SvtoXZYQQgiRgfSMlFBJaUm89MtLXEm6goudCwufWUjtsrW1LksIIYTIRMJICZRqSuW/If/lcsJlAGa3m03jio21LUoIIYTIhnWfplEl83qagRsHEno1FL1Oz4dtPqS5R3OtSxJCCCGyZd1hpIRRSvHZgc8IvRoKwIdPfkjHqh21LUoIIYR4CDlNU4LMOzyPxUcXA/BGkzckiAghhCgWJIyUEMeuH+Or0K8AGNhgIEMaDtG4IiGEECJnJIyUALdSbjHkd3P4cDA48OZjb6LTldx7pwghhChZrDyM3B3AWsyP25/u+5RbKbcAWNJpiQQRIYQQxYqVh5F0xffgHX49nB9P/gjAf5r8h/rl62tckRBCCJE7EkaKsfDr4by84WUAfMv60r9+f20LEkIIIfJAwkgxdermKV797VXSTGk42zrzVcBX2BpstS5LCCGEyDUJI8XQjTs3GLZlGHeMd/Aq5cXarmup6FRR67KEEEKIPMlTGJk7dy5Vq1bFwcEBf39/9u7dm23bBQsW0KZNG8qWLUvZsmUJCAh4YPtCVUzvwDpl1xSuJF6htF1pFnVchEcpD61LEkIIIfIs12Fk5cqVjBkzhsmTJ3PgwAH8/Pzo0KEDV65cybJ9SEgIvXv3Ztu2bezevRsfHx+eeeYZLl68+MjF5xdVjAaw/nL6F7ad3wbArLaz8Hb21rgiIYQQ4tHkOozMmjWLIUOGMGDAAOrVq8e8efNwcnJi0aJFWbZftmwZr7/+Oo0bN6ZOnTosXLgQk8nE1q1bH7l4a/PHuT94Z8c7APTy7cXjno9rXJEQQgjx6HIVRlJSUti/fz8BAQH3VqDXExAQwO7du3O0jsTERFJTUylXrly2bZKTk4mPj8/wsnZGk5E3t70JQDmHcoxrMU7bgoQQQoh8kqswcu3aNYxGI+7u7hmmu7u7Ex0dnaN1jBs3Di8vrwyB5n4zZszA1dXV8vLx8clNmSXS7AOzUXdv0vbJU59go5dnHAohhCgZCvVqmpkzZ7JixQrWrl2Lg4NDtu0mTJhAXFyc5XX+/PlCrLLoOX7jOEuOLQFguN9wmns017YgIYQQIh/l6s/rChUqYDAYiImJyTA9JiYGD48HX9HxySefMHPmTLZs2UKjRo0e2Nbe3h57e/vclJZHRf9qmlRjKj1/6QmARykPXvd7XeOKhBBCiPyVq54ROzs7mjZtmmHwafpg1JYtW2a73EcffcTUqVPZuHEjzZo1y3u1BaboXk3zwd8fWE7PLO24VJ47I4QQosTJ9cCDMWPG0K9fP5o1a0aLFi2YPXs2CQkJDBgwAIBXX30Vb29vZsyYAcCHH37Ie++9x/Lly6lataplbImzszPOzs75+FFKnq1nt7Lu1DoARjYeiaezp7YFCSGEEAUg12EkMDCQq1ev8t577xEdHU3jxo3ZuHGjZVDruXPn0Ovvdbh8/fXXpKSk8OKLL2ZYz+TJk5kyZcqjVV+CKaX47MBnAFRwrMDQRkM1rkgIIYQoGHm6JGPkyJGMHDkyy3khISEZ3kdFReVlE4WiKN+A9f3d73M2/iwAX/3fV3J6RhRrSinS0tIwGo1alyKEyEcGgwEbG5tHPkZZ9fWhRfXwHp0QzY8nfwSgb72+1C1fV+OKhMi7lJQULl++TGJiotalCCEKgJOTE56entjZ2eV5HVYdRtIVpdvBJxuTLTc3A/hv0/9qV4wQj8hkMhEZGYnBYMDLyws7Ozvp5ROihFBKkZKSwtWrV4mMjKRWrVoZhmnkhoSRIubt7W9z7Pox7PR2fPfsdxj0Bq1LEiLPUlJSMJlM+Pj44OTkpHU5Qoh85ujoiK2tLWfPniUlJeWB9xB7kEK96Zl4sA1nNlgegvd+6/fl9IwoMfL615IQoujLj59vq/4NoYrQTc/upN1h3F/m5834e/rTuVpnjSsSQgghCodVh5H0O7BqfQo72ZjM4N8HW95Paz1NzqsLIYSwGlYeRsy07h+ZvGsyh64eQoeOOe3m4F7K/eELCSEK3fTp0y03bHR2dmb69OlalyREiSADWDX2T/Q/rD+zHoCRTUbydOWnNa5ICJGd4cOH07NnT8v7cuXKaViNECWH9Ixo6HTsaQZuGghAowqNGNJwiMYVCSEepFy5ctSsWdPyKg5h5Pr161SsWLFI34CyJOnVqxeffvqp1mUUO1YdRuyTrmq27etJ1+m3sR8ATjZOzGo7S8aJCFHEjB49mu7duz902v369++PTqdDp9Nha2tLtWrVGDt2LHfu3MnQ7vz58wwcONByD5YqVaowatQorl+/nmmd0dHRvPHGG1SvXh17e3t8fHzo0qVLhgeXZmXatGl07dqVqlWrZpq3e/duDAYDnTtnHjDftm1b3nzzzUzTlyxZQpkyZfKtvvvNnTuXqlWr4uDggL+/P3v37n3oMrdu3eLNN9+kSpUqODo60qpVK/75558czwcwGo28++67VKtWDUdHR2rUqMHUqVNR/7pV95QpUyzf1/RXnTp1Mqxn0qRJTJs2jbi4uFx9bmtn1WFE6cz38ChrvFno257691TikuOw1duyqssqGSciRBG0d+/eTE8az2paVjp27Mjly5c5c+YMn332GUFBQUyePNky/8yZMzRr1oyTJ0/y/fffc+rUKebNm2d5CvqNGzcsbaOiomjatCl//PEHH3/8MUeOHGHjxo20a9eOESNGZFtDYmIiwcHBDBo0KMv5wcHBvPHGG/z5559cunTpoZ8pO3mt734rV65kzJgxTJ48mQMHDuDn50eHDh24cuXKA5cbPHgwmzdv5ttvv+XIkSM888wzBAQEcPHixRzNB/NDXb/++mu+/PJLwsPD+fDDD/noo4/44osvMmyrfv36XL582fLasWNHhvkNGjSgRo0afPfddzn+3AJQxUBcXJwCVFxcXL6uN2LZ20pNdlHbZ76Qr+t9mD/P/6kaLGmgGixpoFadWFWo2xaiMCUlJamwsDCVlJSkdSm5kpycrGxsbBTm8e0KUE2aNMk0zd/fP8vl+/Xrp7p27ZphWvfu3VWTJk0s7zt27KgqVaqkEhMTM7S7fPmycnJyUsOHD7dM69Spk/L29la3b9/OtK2bN29m+zl++OEH5ebmluW8W7duKWdnZ3X8+HEVGBiopk2blmH+U089pUaNGpVpucWLFytXV9cM0/Ja3/1atGihRowYYXlvNBqVl5eXmjFjRrbLJCYmKoPBoH799dcM0x977DE1ceLEh85P17lzZzVw4MAMbbp376769OljeT958mTl5+f30M/x/vvvqyeeeOKh7UqKB/2c5/T4bdU9I+nX0SToSxfaFq8lXeOt7W8B0NanLS/VfqnQti2EyBkbGxt27twJQGhoKJcvX2bLli2Zpm3cuDFH6zt69Ci7du2yPLvjxo0bbNq0iddffx1HR8cMbT08POjTpw8rV65EKcWNGzfYuHEjI0aMoFSpUpnWndUpk3R//fUXTZs2zXLeqlWrqFOnDr6+vrzyyissWrQowymJnMppfUuWLHngqeiUlBT2799PQECAZZperycgIIDdu3dnu1z6Axjvv/Ono6MjO3bseOj8dK1atWLr1q1EREQAcOjQIXbs2EGnTp0yLHfy5Em8vLyoXr06ffr04dy5c5lqatGiBXv37iU5OTnbukVGcjUNhfdsmsTURF7Z8AqJaYmUtivNzDYzC2W7QhQVSimSUrV5cq+jrSHH47L0ej2XLl2ifPny+Pn5WaZnNS07v/76K87OzqSlpZGcnIxer+fLL78EzAc0pRR162Z9l+W6dety8+ZNrl69SlRUFEqpTGMTcuLs2bN4eXllOS84OJhXXnkFMJ9SiouLY/v27bRt2zZX2zh16lSO6nN1dcXX1zfb+deuXcNoNOLunvGUtbu7O8ePH892udKlS9OyZUumTp1K3bp1cXd35/vvv2f37t3UrFnzofPTjR8/nvj4eOrUqYPBYMBoNDJt2jT69OljaePv78+SJUvw9fXl8uXLvP/++7Rp04ajR49SuvS9P2q9vLxISUkhOjqaKlWqPHC/CDMrDyOFe4eRqX9P5eJt8znKOe3mUMo2818RQpRkSalG6r23SZNth33QASe7nP/KO3jwYKbQkdW07LRr146vv/6ahIQEPvvsM2xsbOjRo0eGNjnpichLb0W6pKSkLJ8VcuLECfbu3cvatWsBc09QYGAgwcHBuQ4jOa3vhRde4IUXXsjVunPq22+/ZeDAgXh7e2MwGHjsscfo3bs3+/fvz9F8MPcULVu2jOXLl1O/fn1CQ0N588038fLyol8/88UG/+4ladSoEf7+/lSpUoVVq1ZlGJeT3tslT6rOOSs/TZOu4HtGTtw4wa9nfgVg+hPTae7RvMC3KYTIu9DQ0EzBI6tp2SlVqhQ1a9bEz8+PRYsWsWfPHoKDgwGoWbMmOp2O8PDwLJcNDw+nbNmyuLm5UatWLXQ63QN7B7JToUIFbt7MPEA/ODiYtLQ0vLy8sLGxwcbGhq+//poff/zRchWIi4tLlleExMbG4urqann/KPXdX6vBYCAmJibD9JiYGDw8PB64bI0aNdi+fTu3b9/m/Pnz7N27l9TUVKpXr56j+QBvv/0248ePp1evXjRs2JC+ffsyevRoZsyYke12y5QpQ+3atTl16lSG6emDj93c3HK1D6yZdfeMPMJfHLmRbEzmxV9eBKCKSxW61OhSKNsVoqhxtDUQ9kEHzbadG0eOHMnUk5HVtJzQ6/W88847jBkzhpdffpny5cvTvn17vvrqK0aPHp1h3Eh0dDTLli3j1VdfRafTUa5cOTp06MDcuXP5z3/+k2lcRmxsbLbjRpo0aZLpqo60tDSWLl3Kp59+yjPPPJNhXrdu3fj+++8ZPnw4vr6+/P7775nWeeDAAWrXrm15/yj1/ZudnR1NmzZl69atdOvWDQCTycTWrVsZOXLkQ5cHcwAsVaoUN2/eZNOmTXz00Uc5np+YmJjpgW8GgwGTyZTt9m7fvs3p06fp27dvhulHjx6lUqVKVKhQIUd1C6z8aprvxig12UX9+nH/fF3v/QZtHKQaLGmgGn/TWJ2PP1+g2xKiKCmuV9MopVSVKlXUO++8oy5evKhiY2OznZaVrK6mSU1NVd7e3urjjz9WSikVERGhKlSooNq0aaO2b9+uzp07p3777TfVoEEDVatWLXX9+nXLsqdPn1YeHh6qXr16avXq1SoiIkKFhYWpOXPmqDp16mRbx+HDh5WNjY26ceOGZdratWuVnZ1dlvWPHTtWNWvWzLJNBwcH9cYbb6hDhw6p48ePq08//VTZ2Nio3377LcNyOalvzZo1ytfXN9talVJqxYoVyt7eXi1ZskSFhYWpoUOHqjJlyqjo6GhLmy+++EI9/fTTGZbbuHGj+u2339SZM2fU77//rvz8/JS/v79KSUnJ0XylzN8zb29v9euvv6rIyEi1Zs0aVaFCBTV27FhLm//+978qJCRERUZGqp07d6qAgABVoUIFdeXKlQz19OvXL9OVOSVZflxNY+VhZHSBh5H5h+ZbLuNdeXxlgW1HiKKoOIeRb7/9Vnl5eSlAvfXWW9lOy0pWYUQppWbMmKHc3Nwsl8BGRUWpfv36KXd3d2Vra6t8fHzUG2+8oa5du5Zp2UuXLqkRI0aoKlWqKDs7O+Xt7a2ef/55tW3btgd+jhYtWqh58+ZZ3j/33HPq2WefzbLtnj17FKAOHTqklFJq7969qn379srNzU25uroqf39/tXbt2iyXfVh9ixcvVjn5+/eLL75QlStXVnZ2dqpFixbq77//zjB/8uTJqkqVKhmmrVy5UlWvXl3Z2dkpDw8PNWLEiAxh62HzlVIqPj5ejRo1SlWuXFk5ODio6tWrq4kTJ6rk5GRLm8DAQOXp6Wn5fIGBgerUqVMZ1pOUlKRcXV3V7t27H/pZS4r8CCM6pQrpXMUjiI+Px9XVlbi4OFxcXPJtvSeXjaHWyWB+de7Bc28tyrf1pvsn+h9e2/IaycZkXqj5Ah+0/iDftyFEUXbnzh0iIyOpVq1algMpRcFbv349b7/9NkePHs10GkLkv6+//pq1a9dmeYqrpHrQz3lOj98yZoSCGb4aFRfFiK0jSDYm4+/pz5RWUwpgK0II8WCdO3fm5MmTXLx4ER8fH63LKfFsbW0z3bVVPJx1h5G78vs+I0lpSfT4uQcpphS8nb35vN3n6HXyF4kQQhtZPWNGFIzBgwdrXUKxZOVHyII5QzV512RSTCkAzG8/HydbpwLZjhBCCFESWHkYSZd/PSMHYg7wW+RvAPynyX+o7FI539YthBBClERWHUZ0BdAzMmrbKAB8y/oyuKF01wkhhBAPY9VhJF1+jRk5eOUgscmxALzX8r0cPwdDCCGEsGbWHUby8armxNRE3tv5HgD+Hv40cmuUb+sWQgghSjLrDiPp8qEH45N9nxAVHwXAxMcnPvL6hBBCCGshYSQfrD25lh8ifgBgauupVHOtpnFFQgghRPFh3WHk7mmaRzlZczvlNlN2TwGgbaW2dK3R9dHrEkIIIayIdYcRi7yfplkathSTMqHX6Zn55EwZtCqEEHc9+eSTLF++XOsySryUlBSqVq3Kvn37tC4lz6w6jDzqpb037txgadhSAP7b9L+Usi31kCWEEMVF//79LY+yf5iQkBB0Oh2xsbF5ble1alVmz55tafOgV0hISJbr/3cbFxcXmjdvzk8//ZSpXVJSEpMnT6Z27drY29tToUIFXnrpJY4dO5apbXx8PBMnTqROnTo4ODjg4eFBQEAAa9as4UGPNvv555+JiYmhV69emebNmDEDg8HAxx9/nGnelClTaNy4cabpUVFR6HQ6QkNDLdOUUsyfPx9/f3+cnZ0pU6YMzZo1Y/bs2SQmJmZbW1Z++OEHy2ds2LAhGzZseOgyc+fOpW7dujg6OuLr68vSpUszzE9NTeWDDz6gRo0aODg44Ofnx8aNGzO0+fPPP+nSpQteXl7odDrWrVuXaTsxMTH0798fLy8vnJyc6NixIydPnrTMt7Oz46233mLcuHG5+sxFiVWHkXR5vbR34o6JJKQmUN6hPC/WfjGfqxJCWKNWrVpx+fJly6tnz5507Ngxw7RWrVplu/zixYu5fPky+/bto3Xr1rz44oscOXLEMj85OZmAgAAWLVrE//73PyIiItiwYQNpaWn4+/vz999/W9rGxsbSqlUrli5dyoQJEzhw4AB//vkngYGBjB07lri4uGzr+PzzzxkwYECWD+dbtGgRY8eOZdGiR3tAad++fXnzzTfp2rUr27ZtIzQ0lHfffZeffvopVw+q27VrF71792bQoEEcPHiQbt260a1bN44ePZrtMl9//TUTJkxgypQpHDt2jPfff58RI0bwyy+/WNpMmjSJoKAgvvjiC8LCwhg+fDgvvPACBw8etLRJSEjAz8+PuXPnZrkdpRTdunXjzJkz/PTTTxw8eJAqVaoQEBBAQkKCpV2fPn3YsWNHloGyWCiApwnnu5w+gji3Tn3zulKTXdS6T4bletk9l/aoBksaqAZLGqht57bla11ClBQPerR4UdevXz/VtWtXpZRSRqNRTZ8+XVWtWlU5ODioRo0aqR9++EEppVRkZKTCPPTM8urXr1+W69y2bZsC1M2bNzPNq1Klivrss88eWMfDAGrt2rWW9/Hx8QpQc+bMsUybOXOm0ul0KjQ0NMOyRqNRNWvWTNWrV0+ZTCallFKvvfaaKlWqlLp48WKmbd26dUulpqZmWceVK1eUTqdTR48ezTQvJCREeXt7q5SUFOXl5aV27tyZYf7kyZOVn59fpuXS9/PBgweVUkqtXLlSAWrdunWZ2ppMJhUbG5tlbVnp2bOn6ty5c4Zp/v7+atiw7I8NLVu2VG+99VaGaWPGjFGtW7e2vPf09FRffvllhjbdu3dXffr0yXKd93//lFLqxIkTCsiwL41Go3Jzc1MLFizI0LZdu3Zq0qRJ2dZcUB70c57T47dV94wkJKeZv8hDx8gbf7wBmAettvVpm39FCVGSKQUpCdq8HuG+QjNmzGDp0qXMmzePY8eOMXr0aF555RW2b9+Oj48PP/74IwAnTpzg8uXLzJkzJ7/2WJ6lpaURHBwMmLvx0y1fvpz27dvj5+eXob1er2f06NGEhYVx6NAhTCYTK1asoE+fPnh5eWVav7OzMzY2WT9rdceOHTg5OVG3bt1M84KDg+nduze2trb07t3bUmNuLVu2DF9fX7p2zXzRgE6nw9XVFbh3aiwqKirbde3evZuAgIAM0zp06MDu3buzXSY5ORkHB4cM0xwdHdm7dy+pqakPbLNjx44Hfrb7twNkWI9er8fe3j7Telq0aMFff/2V43UXJVb91F57G3MWu3YrJVfLnbhxgsQ08/nINx57I9/rEqLESk2E6ZkPbIXinUtgl/txXcnJyUyfPp0tW7bQsmVLAKpXr86OHTsICgriqaeeoly5cgBUrFiRMmXKPHSdlSpVyjQtt2McstO7d28MBgNJSUmYTCaqVq1Kz549LfMjIiJo165dlsumh4eIiAi8vLy4efMmderUyXUNZ8+exd3dPdMpmvj4eFavXm05yL/yyiu0adOGOXPm4OzsnKttnDx5El9f34e2c3JywtfXF1tb22zbREdH4+7unmGau7s70dHR2S7ToUMHFi5cSLdu3XjsscfYv38/CxcuJDU1lWvXruHp6UmHDh2YNWsWTz75JDVq1GDr1q2sWbMGo9GY489Zp04dKleuzIQJEwgKCqJUqVJ89tlnXLhwgcuXL2do6+XlxdmzZ3O87qLEqntG0rm7Oua4rdFk5D9//MfyvnbZ2gVRkhCiiDh16hSJiYm0b98eZ2dny2vp0qWcPn06T+v866+/CA0NzfDKqvchLz777DNCQ0P57bffqFevHgsXLrSEpXQqB71EOWmTnaSkpEw9AgDff/89NWrUsPTKNG7cmCpVqrBy5cpcbyOn9bVo0YLjx4/j7e2d6208yLvvvkunTp14/PHHsbW1pWvXrvTr1w/AEsLmzJlDrVq1qFOnDnZ2dowcOTLbcTTZsbW1Zc2aNURERFCuXDmcnJzYtm0bnTp1yrQeR0fHfAu1hc2qe0bycoeR9ZHruZRwCb1Oz5KOS/K/JCFKMlsncw+FVtvOg9u3bwOwfv36TAc0e3v7PK2zWrVqmXpQsjvlkVseHh7UrFmTmjVrsnjxYp599lnCwsKoWLEiALVr1yY8PDzLZdOn165dGzc3N8qUKcPx48dzXUOFChW4efNmpunBwcEcO3Ysw2c1mUwsWrSIQYMGAeDi4pLlwNj0K5DST7/Url07T7VlxcPDg5iYmAzTYmJi8PDwyHYZR0dHFi1aRFBQEDExMXh6ejJ//nxKly6Nm5sbAG5ubqxbt447d+5w/fp1vLy8GD9+PNWrV89VfU2bNiU0NJS4uDhSUlJwc3PD39+fZs2aZWh348YNy7aLG6vuGbEE6xyOGUlMTeT9Xe8DMLDBQJpUbFIwhQlRUul05lMlWrzyeA+gevXqYW9vz7lz5ywH+fSXj48PcG9MRm663wtDixYtaNq0KdOmTbNM69WrF1u2bOHQoUMZ2ppMJj777DPq1auHn58fer2eXr16sWzZMi5dyhwgb9++TVpaWpbbbdKkCdHR0RkCyZEjR9i3bx8hISEZeoRCQkLYvXu3JVj4+vpy4cKFTOHgwIEDODg4ULlyZQBefvllIiIisrx0WSn1wCt97teyZUu2bt2aYdrmzZstp+UexNbWlkqVKmEwGFixYgXPPfdcph4LBwcHvL29SUtL48cff8xynEtOuLq64ubmxsmTJ9m3b1+m9Rw9epQmTYrnccmqw0h6z4guh2nk3Z3vkmJKwVZvS6BvYEEWJoQoIkqXLs1bb73F6NGj+eabbzh9+jQHDhzgiy++4JtvvgGgSpUq6HQ6fv31V65evWrpTSkK3nzzTYKCgrh48SIAo0ePpkWLFnTp0oUffviBc+fO8c8//9CjRw/Cw8MJDg623Lxx2rRp+Pj44O/vz9KlSwkLC+PkyZMsWrSIJk2aZPs5mzRpQoUKFdi5c6dlWnBwMC1atODJJ5+kQYMGlteTTz5J8+bNLQNZO3TogK+vL71792bXrl2cOXOG1atXM2nSJEaNGoXBYACgZ8+eBAYG0rt3b6ZPn86+ffs4e/Ysv/76KwEBAWzbtg2AvXv3UqdOHcvnz8qoUaPYuHEjn376KcePH2fKlCns27ePkSNHWtpMmDCBV1991fI+IiKC7777jpMnT7J371569erF0aNHmT59uqXNnj17WLNmDWfOnOGvv/6iY8eOmEwmxo4da2lz+/ZtSzADiIyMJDQ0lHPnzlna/PDDD4SEhFgu723fvj3dunXjmWeeyfA5/vrrr0zTio2CuMwnvxXUpb3hC4coNdlF/Tp75EPbpqSlWC7l/d/u/+VrHUKUVMX50t6+ffuqHj16KKXMl4rOnj1b+fr6KltbW+Xm5qY6dOigtm/fbmn/wQcfKA8PD6XT6YrMpb3ptdepU0e99tprlmkJCQlq4sSJqmbNmsrW1laVK1dO9ejRQx05ciTTOmNjY9X48eNVrVq1lJ2dnXJ3d1cBAQFq7dq1lkuAszJ27FjVq1cvpZRSycnJqnz58uqjjz7Ksu2HH36oKlasqFJSUpRSSl28eFH169dPVa5cWTk6Oqp69eqpmTNnWuanMxqN6uuvv1bNmzdXTk5OysXFRTVt2lTNmTNHJSYmKqXu7fPIyMgH7rtVq1ap2rVrKzs7O1W/fn21fv36DPP79eunnnrqKcv7sLAw1bhxY+Xo6KhcXFxU165d1fHjxzMsExISourWravs7e1V+fLlVd++fTNdJp1e3/2vf/8fmjNnjqpUqZKytbVVlStXVpMmTVLJyckZ1rNr1y5VpkwZy+cuTPlxaa9OqUcYpVRI4uPjcXV1JS4uDhcXl3xb7/HgodQ5v5L15V6l83++eGDbiTsm8vPpnwHY1nMbFRwr5FsdQpRUd+7cITIykmrVqmU5oLEo69ixIzVr1uTLL7/UupRiKTo6mvr163PgwAGqVKmidTklXmBgIH5+frzzzjuFvu0H/Zzn9Pht1adpVA4HjYRdD7MEkRdrvyhBRIgS7ObNm/z666+EhIRkuveEyDkPDw+Cg4MznG4QBSMlJYWGDRsyevRorUvJM7mahgdHkci4SAJ/NY8PcbV35d3H3y2EuoQQWhk4cCD//PMP//3vf/M80FCY5fTZPuLR2NnZMWnSJK3LeCRWHkbuesAo+2l7zKPQXe1dWdppKXqdVXcmCVHirV27VusShLA61n1kfchomQ1nNrDn8h4Avnz6S6q75u7acCGEEEI8nHWHkfQ0kkXPyIGYA4z7y/w45m41u9G4YuNCrEsIIYSwHhJGyHrMyHu73gOgnEM5Jj1evM/FCSGEEEWZdYcRy2majHHk27BvORtvftjQW83ewt6Qt1s+CyGEEOLhrDqMKMtpmnvTYu/E8tE/HwHQyK0RXWp00aAyIYQQwnpYdRi5514aWX1yteXrhc8s1KIYIYQQwqpYdxhRmXtG5hyYA8DzNZ7H0cZRg6KEEEII62LVYeTelb3mNHLwykHLlEENBhV6PUKIkmX+/Pn4+Pig1+uZPXu21uWUGE8++STLly/XuowSLywsjEqVKpGQkFDg27LqMJLeM6IDElMTeW+n+Qqa0nalqV5G7ikihLW6evUqr732GpUrV8be3h4PDw86dOiQ4Sm0DxMfH8/IkSMZN24cFy9eZOjQobRt25Y333wzR8sfO3aMnj174ubmhr29PbVr1+a9994jMTExU9uDBw/y0ksv4e7ujoODA7Vq1WLIkCFEREQAEBUVhU6ns7xKly5N/fr1GTFiBCdPnnxoLf9e1sXFhebNm/PTTz9lapeUlMTkyZOpXbs29vb2VKhQgZdeeoljx45luX8mTpxInTp1cHBwwMPDg4CAANasWcODHpn2888/ExMTQ69evTLNmzFjBgaDgY8//jjTvClTptC4ceNM09P3TfpTc8H8qJD58+fj7++Ps7MzZcqUoVmzZsyePTvL/f8gP/zwg+UzNmzYkA0bNjx0meTkZCZOnEiVKlWwt7enatWqLFq0yDL/2LFj9OjRg6pVq6LT6bIMuunz7n+NGDHC0mbYsGHUqFEDR0dH3Nzc6Nq1K8ePH7fMr1evHo8//jizZs3K1WfOC+sOIxY6Pj/4OVHxUdjqbVnacanWBQkhNNSjRw8OHjzIN998Q0REBD///DNt27bl+vXrOV7HuXPnSE1NpXPnznh6euLk5JTjZf/++2/8/f1JSUlh/fr1REREMG3aNJYsWUL79u1JSUmxtP311195/PHHSU5OZtmyZYSHh/Pdd9/h6urKu+9mfHzFli1buHz5MocOHWL69OmEh4fj5+fH1q1bH1rT4sWLuXz5Mvv27aN169a8+OKLHDlyxDI/OTmZgIAAFi1axP/+9z8iIiLYsGEDaWlp+Pv78/fff1vaxsbG0qpVK5YuXcqECRM4cOAAf/75J4GBgYwdO5a4uLhs6/j8888ZMGAAen3mw9eiRYsYO3ZshgN3XvTt25c333yTrl27sm3bNkJDQ3n33Xf56aef+P3333O8nl27dtG7d28GDRrEwYMH6datG926dePo0aMPXK5nz55s3bqV4OBgTpw4wffff4+vr69lfmJiItWrV2fmzJl4eHhkuY5//vmHy5cvW16bN28G4KWXXrK0adq0KYsXLyY8PJxNmzahlOKZZ57BaDRa2gwYMICvv/6atLS0HH/uPMn/hwnnv5w+gji3jn7VV6nJLmrN1/9RDZY0UA2WNFCrT6zO120IYc3uf7S4yWRSCSkJmrwe9Lj7f7t586YCVEhIyAPbnT17Vj3//POqVKlSqnTp0uqll15S0dHRSimlFi9enOUj4e+fltVj7U0mk6pXr55q1qyZMhqNGeaFhoYqnU6nZs6cqZRSKiEhQVWoUEF169Yt28+ilFKRkZEKUAcPHsww32g0qrZt26oqVaqotLS0bD8roNauXWt5Hx8frwA1Z84cy7SZM2cqnU6nQkNDM22jWbNmql69epbvwWuvvaZKlSqlLl68mGlbt27dUqmpqVnWceXKFaXT6dTRo0czzQsJCVHe3t4qJSVFeXl5qZ07d2aYP3nyZOXn55dpufv3zcqVKxWg1q1bl6mtyWRSsbGxWdaWlZ49e6rOnTtnmObv76+GDRuW7TK//fabcnV1VdevX8/RNqpUqaI+++yzh7YbNWqUqlGjxgN/Dg4dOqQAderUKcu05ORkZW9vr7Zs2ZLtcvf/nP9bTo/feXo2zdy5c/n444+Jjo7Gz8+PL774ghYtWmTb/ocffuDdd98lKiqKWrVq8eGHH/Lss8/mZdP5Ku5OKgCL7e6NFelaUx6MJURBSUpLwn+5vybb3vPyHpxsH9474ezsjLOzM+vWrePxxx/H3j7zfYZMJhNdu3bF2dmZ7du3k5aWxogRIwgMDCQkJITAwEB8fHwICAhg7969+Pj44OjoSEREBA0aNOCDDz4AwM3NLdO6Q0NDCQsLY/ny5Zn++vfz8yMgIIDvv/+ecePGsWnTJq5du8bYsWOz/CxlypR54GfV6/WMGjWKF154gf379z/w93i6tLQ0goODAfMD2tItX76c9u3b4+fnl2kbo0ePpk+fPhw6dIhGjRqxYsUK+vTpg5eXV6b1Ozs7Z7vtHTt24OTkRN26dTPNCw4Opnfv3tja2tK7d2+Cg4Np1arVQz/P/ZYtW4avr2+WD0nU6XS4uroCEBISQrt27YiMjKRq1apZrmv37t2MGTMmw7QOHTqwbt26bLf/888/06xZMz766CO+/fZbSpUqxfPPP8/UqVNxdMzbRRUpKSl89913jBkzBl02z2JLSEhg8eLFVKtWDR8fH8t0Ozs7GjduzF9//cX//d//5Wn7OZHr0zQrV65kzJgxTJ48mQMHDuDn50eHDh24cuVKlu3z2k1VGErZGjhha0uk4SYA77d6Hxu9PDtQCGtmY2PDkiVL+OabbyhTpgytW7fmnXfe4fDhw5Y2W7du5ciRIyxfvpymTZvi7+/P0qVL2b59O//88w+Ojo6UL18eMAcODw8PXF1dsbOzw8nJCQ8PDzw8PDAYDJm2nz7OI6sDbvr09Dbp4z3q1KmT58+bvmxUVNQD2/Xu3RtnZ2fs7e0ZPXo0VatWpWfPnhnqflDN6W2uXbvGzZs381Tz2bNncXd3zxTS4uPjWb16Na+88goAr7zyCqtWreL27du53sbJkycznBLJjpOTE76+vtja2mbbJjo6Gnd39wzT3N3diY6OznaZM2fOsGPHDo4ePcratWuZPXs2q1ev5vXXX8/5h7jPunXriI2NpX///pnmffXVV5YA/ttvv7F58+YMIRPAy8uLs2fP5nn7OZHrI++sWbMYMmQIAwYMAGDevHmsX7+eRYsWMX78+Ezt58yZQ8eOHXn77bcBmDp1Kps3b+bLL79k3rx5j1j+o9EZk3i/QjnL++61umtYjRAln6ONI3te3qPZtnOqR48edO7cmb/++ou///6b3377jY8++oiFCxfSv39/wsPD8fHxyfAXZL169ShTpgzh4eE0b978ketVDxjEmX6weFCb3G4nu7+Y03322WcEBARw5swZRo8ezeeff065cuUytMlJPY9Sc1JSEg4ODpmmf//999SoUcPSK9O4cWOqVKnCypUrGTQod1dG5rS+Fi1aZBjsmV9MJhM6nY5ly5ZZemFmzZrFiy++yFdffZWn3pHg4GA6deqUZU9Unz59aN++PZcvX+aTTz6hZ8+e7Ny5M8N+dnR0zPXA3dzKVc9ISkoK+/fvJyAg4N4K9HoCAgLYvXt3lsvs3r07Q3swd1Nl1x7MA6Hi4+MzvArCFuNljjiYu2A/eeqTAtmGEOIenU6Hk62TJq+HHWzv5+DgQPv27Xn33XfZtWsX/fv3Z/LkyQW0Z+6pVasWAOHh4VnODw8Pp3bt2gCWfx/loJi+nWrVqj2wnYeHBzVr1uSZZ55h8eLFBAYGZugRr1279gNrTm/j5uZGmTJl8lRzhQoVuHnzZqbpwcHBHDt2DBsbG8srLCwsw0BWFxeXLAfGxsbGAlgO/LVr1863kOHh4UFMTEyGaTExMdkOOgXw9PTE29vbUg+Ye5aUUly4cCHXNZw9e5YtW7YwePDgLOe7urpSq1YtnnzySVavXs3x48dZu3ZthjY3btzI8pRifspVGLl27RpGozFX3U556aaaMWMGrq6ulte///rIL0lpSfxY5hYAT9s9RoeqHfJ9G0KIkqNevXqW+y3UrVuX8+fPc/78ecv8sLAwYmNjqVevXrbrsLOzy3ClQlaaNGlCnTp1+OyzzzCZTBnmHTp0iC1btli625955hkqVKjARx99lOW60g+02TGZTHz++edUq1aNJk2aPLDtv7Vo0YKmTZsybdo0y7RevXqxZcsWDh06lGkbn332GfXq1cPPzw+9Xk+vXr1YtmwZly5dyrTu27dvZ3vlRpMmTYiOjs4QSI4cOcK+ffsICQkhNDTU8goJCWH37t2WYOHr68uFCxcyhYMDBw7g4OBA5cqVAXj55ZeJiIjI8tJlpdQDr/S5X8uWLTNdqbR582ZatmyZ7TKtW7fm0qVLGU4xRUREoNfrqVSpUo63nW7x4sVUrFiRzp07P7StUgqlFMnJyRmmHz16NFf/P/LkgcNb73Px4kUFqF27dmWY/vbbb6sWLVpkuYytra1avnx5hmlz585VFStWzHY7d+7cUXFxcZbX+fPn8/1qGpPJpFb/+I56ef6T6tq5w/m2XiHEPQ8aZV9UXbt2TbVr1059++236tChQ+rMmTNq1apVyt3dXQ0cOFApZf790bhxY9WmTRu1f/9+tWfPHtW0aVP11FNPWdZz8ODBTFfMDBkyRDVv3lxFRkaqq1evZrpaJt2OHTuUk5OT6tatm9qzZ486e/asWrVqlfLx8VEdO3bMcOXLunXrlK2trerSpYvavHmzioyMVP/88496++23VWBgoFLq3hUjW7ZsUZcvX1anT59WP/30k2rXrp1ydHRUf/zxxwP3CfddTaOUUhs2bFD29vbqwoULSinz99rf31/5+PioVatWqbNnz6q9e/eqbt26qVKlSqndu3dblr1+/bqqU6eOqlSpkvrmm2/UsWPHVEREhAoODlY1a9a0XAV0v7S0NOXm5qZ++eUXy7RRo0Ypf3//LNu3aNFCvfXWW0oppVJTU1X9+vVVu3bt1M6dO9Xp06fVDz/8oDw9PdW4ceMsy5hMJhUYGKgcHR3VtGnT1D///KOioqLUL7/8op5++mnLftizZ4/y9fW1fP6s7Ny5U9nY2KhPPvlEhYeHq8mTJytbW1t15MgRS5vx48ervn37Wt7funVLVapUSb344ovq2LFjavv27apWrVpq8ODBljbJycnq4MGD6uDBg8rT01O99dZb6uDBg+rkyZMZtm80GlXlypUzfL50p0+fVtOnT1f79u1TZ8+eVTt37lRdunRR5cqVUzExMZZ2kZGRSqfTqaioqGw/Z35cTZOrMJKcnKwMBkOm/5Svvvqqev7557NcxsfHJ9NlR++9955q1KhRjrdbUJf2CiEKVnEMI3fu3FHjx49Xjz32mHJ1dVVOTk7K19dXTZo0SSUmJlraPejSXqWyDiMnTpxQjz/+uHJ0dMz20t50hw8fVj169FDlypWzXAo8cuTILC97/eeff1T37t2Vm5ubsre3VzVr1lRDhw61HJzSw0j6y8nJSdWtW1e9/vrrmQ5gWckqjJhMJlWnTh312muvWaYlJCSoiRMnqpo1aypbW1tVrlw51aNHjwwH33SxsbFq/PjxqlatWsrOzk65u7urgIAAtXbt2gdefjp27FjVq1cvpZT5mFS+fHn10UcfZdn2ww8/VBUrVlQpKSlKKfMf1P369VOVK1dWjo6Oql69emrmzJmW+emMRqP6+uuvVfPmzZWTk5NycXFRTZs2VXPmzLH8H9i2bdtDv4dKKbVq1SpVu3ZtZWdnp+rXr6/Wr1+fYX6/fv0yhFillAoPD1cBAQHK0dFRVapUSY0ZMybD/737v5/pr/vXs2nTJgWoEydOZKrr4sWLqlOnTqpixYrK1tZWVapUSb388svq+PHjGdpNnz5ddejQ4YGfMT/CiE6p3I0m8vf3p0WLFnzxxReAuQuucuXKjBw5MssBrIGBgSQmJvLLL79YprVq1YpGjRrleABrfHw8rq6uxMXF4eLikptyhRAaunPnDpGRkVSrVi3LgYciZ0wmE4MGDWLTpk1s377dMq7EGkVHR1O/fn0OHDhAlSpVtC6nREtJSaFWrVosX76c1q1bZ9vuQT/nOT1+5/rS3jFjxrBgwQK++eYbwsPDee2110hISLBcXfPqq68yYcIES/tRo0axceNGPv30U44fP86UKVPYt28fI0eOzO2mhRDCKun1eoKDgxk3bhx//fWX1uVoysPDg+DgYM6dO6d1KSXeuXPneOeddx4YRPJLri/tDQwM5OrVq7z33ntER0fTuHFjNm7caBmkeu7cuQzXgLdq1Yrly5czadIk3nnnHWrVqsW6deto0KBB/n0KIYQo4dJvUCagW7duWpdgFWrWrEnNmjULZVu5Pk2jBTlNI0TxJKdphCj5NDlNI4QQQgiRnySMCCEK3P33yxBClBz58fMtD2IRQhQYOzs79Ho9ly5dws3NDTs7u1zfCVUIUTQppUhJSeHq1avo9fpMz7TJDQkjQogCo9frqVatGpcvX87ybptCiOLPycmJypUrZ3qAYW5IGBFCFCg7OzsqV65MWlraQ2+FLoQoXgwGAzY2No/c4ylhRAhR4HQ6Hba2tg983LoQwnrJAFYhhBBCaErCiBBCCCE0JWFECCGEEJoqFmNG0m8SGx8fr3ElQgghhMip9OP2w272XizCyK1btwDw8fHRuBIhhBBC5NatW7dwdXXNdn6xeDaNyWTi0qVLlC5dOl9vmBQfH4+Pjw/nz5+XZ94UINnPhUf2deGQ/Vw4ZD8XjoLcz0opbt26hZeX1wPvQ1Isekb0ej2VKlUqsPW7uLjIf/RCIPu58Mi+LhyynwuH7OfCUVD7+UE9IulkAKsQQgghNCVhRAghhBCasuowYm9vz+TJk7G3t9e6lBJN9nPhkX1dOGQ/Fw7Zz4WjKOznYjGAVQghhBAll1X3jAghhBBCexJGhBBCCKEpCSNCCCGE0JSEESGEEEJoqsSHkblz51K1alUcHBzw9/dn7969D2z/ww8/UKdOHRwcHGjYsCEbNmwopEqLt9zs5wULFtCmTRvKli1L2bJlCQgIeOj3RdyT2//T6VasWIFOp6Nbt24FW2AJkdv9HBsby4gRI/D09MTe3p7atWvL748cyO1+nj17Nr6+vjg6OuLj48Po0aO5c+dOIVVbPP3555906dIFLy8vdDod69ate+gyISEhPPbYY9jb21OzZk2WLFlSsEWqEmzFihXKzs5OLVq0SB07dkwNGTJElSlTRsXExGTZfufOncpgMKiPPvpIhYWFqUmTJilbW1t15MiRQq68eMntfn755ZfV3Llz1cGDB1V4eLjq37+/cnV1VRcuXCjkyouf3O7rdJGRkcrb21u1adNGde3atXCKLcZyu5+Tk5NVs2bN1LPPPqt27NihIiMjVUhIiAoNDS3kyouX3O7nZcuWKXt7e7Vs2TIVGRmpNm3apDw9PdXo0aMLufLiZcOGDWrixIlqzZo1ClBr1659YPszZ84oJycnNWbMGBUWFqa++OILZTAY1MaNGwusxhIdRlq0aKFGjBhheW80GpWXl5eaMWNGlu179uypOnfunGGav7+/GjZsWIHWWdzldj/fLy0tTZUuXVp98803BVViiZGXfZ2WlqZatWqlFi5cqPr16ydhJAdyu5+//vprVb16dZWSklJYJZYIud3PI0aMUE8//XSGaWPGjFGtW7cu0DpLkpyEkbFjx6r69etnmBYYGKg6dOhQYHWV2NM0KSkp7N+/n4CAAMs0vV5PQEAAu3fvznKZ3bt3Z2gP0KFDh2zbi7zt5/slJiaSmppKuXLlCqrMEiGv+/qDDz6gYsWKDBo0qDDKLPbysp9//vlnWrZsyYgRI3B3d6dBgwZMnz4do9FYWGUXO3nZz61atWL//v2WUzlnzpxhw4YNPPvss4VSs7XQ4lhYLB6UlxfXrl3DaDTi7u6eYbq7uzvHjx/Pcpno6Ogs20dHRxdYncVdXvbz/caNG4eXl1em//wio7zs6x07dhAcHExoaGghVFgy5GU/nzlzhj/++IM+ffqwYcMGTp06xeuvv05qaiqTJ08ujLKLnbzs55dffplr167xxBNPoJQiLS2N4cOH88477xRGyVYju2NhfHw8SUlJODo65vs2S2zPiCgeZs6cyYoVK1i7di0ODg5al1Oi3Lp1i759+7JgwQIqVKigdTklmslkomLFisyfP5+mTZsSGBjIxIkTmTdvntallSghISFMnz6dr776igMHDrBmzRrWr1/P1KlTtS5NPKIS2zNSoUIFDAYDMTExGabHxMTg4eGR5TIeHh65ai/ytp/TffLJJ8ycOZMtW7bQqFGjgiyzRMjtvj59+jRRUVF06dLFMs1kMgFgY2PDiRMnqFGjRsEWXQzl5f+0p6cntra2GAwGy7S6desSHR1NSkoKdnZ2BVpzcZSX/fzuu+/St29fBg8eDEDDhg1JSEhg6NChTJw4Eb1e/r7OD9kdC11cXAqkVwRKcM+InZ0dTZs2ZevWrZZpJpOJrVu30rJlyyyXadmyZYb2AJs3b862vcjbfgb46KOPmDp1Khs3bqRZs2aFUWqxl9t9XadOHY4cOUJoaKjl9fzzz9OuXTtCQ0Px8fEpzPKLjbz8n27dujWnTp2yhD2AiIgIPD09JYhkIy/7OTExMVPgSA+ASh6zlm80ORYW2NDYImDFihXK3t5eLVmyRIWFhamhQ4eqMmXKqOjoaKWUUn379lXjx4+3tN+5c6eysbFRn3zyiQoPD1eTJ0+WS3tzILf7eebMmcrOzk6tXr1aXb582fK6deuWVh+h2Mjtvr6fXE2TM7ndz+fOnVOlS5dWI0eOVCdOnFC//vqrqlixovrf//6n1UcoFnK7nydPnqxKly6tvv/+e3XmzBn1+++/qxo1aqiePXtq9RGKhVu3bqmDBw+qgwcPKkDNmjVLHTx4UJ09e1YppdT48eNV3759Le3TL+19++23VXh4uJo7d65c2vuovvjiC1W5cmVlZ2enWrRoof7++2/LvKeeekr169cvQ/tVq1ap2rVrKzs7O1W/fn21fv36Qq64eMrNfq5SpYoCMr0mT55c+IUXQ7n9P/1vEkZyLrf7edeuXcrf31/Z29ur6tWrq2nTpqm0tLRCrrr4yc1+Tk1NVVOmTFE1atRQDg4OysfHR73++uvq5s2bhV94MbJt27Ysf+em79t+/fqpp556KtMyjRs3VnZ2dqp69epq8eLFBVqjTinp2xJCCCGEdkrsmBEhhBBCFA8SRoQQQgihKQkjQgghhNCUhBEhhBBCaErCiBBCCCE0JWFECCGEEJqSMCKEEEIITUkYEUIIIYSmJIwIIYQQQlMSRoQQQgihKQkjQgghhNCUhBEhhBBCaOr/ASIBTQTiwEj2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Exercise: Evaluate the model you made in the previous section\n",
    "#\n",
    "\n",
    "def make_roc(background_scores, signal_scores, legend_label) -> None:\n",
    "    y_true = np.append(\n",
    "        np.ones(len(signal_scores)),\n",
    "        np.zeros(len(background_scores)),\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    y_pred = np.append(\n",
    "        signal_scores,\n",
    "        background_scores,\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=f'{legend_label} ROC (AUC: {auc:.4})'\n",
    "    )\n",
    "\n",
    "zerobias_predictions = conv_ae.predict(zerobias_test)\n",
    "ttbar_predictions = conv_ae.predict(ttbar_data)\n",
    "jetht_predictions = conv_ae.predict(jetht_data)\n",
    "softqcd_predictions = conv_ae.predict(softqcd_data)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_pred-y_true)**2, axis=(1,2,3))\n",
    "\n",
    "zerobias_score = np.array(mean_squared_error(zerobias_test, zerobias_predictions))\n",
    "ttbar_score = np.array(mean_squared_error(ttbar_data, ttbar_predictions))\n",
    "jetht_score = np.array(mean_squared_error(jetht_data, jetht_predictions))\n",
    "softqcd_score = np.array(mean_squared_error(softqcd_data, softqcd_predictions))\n",
    "\n",
    "make_roc(zerobias_score, ttbar_score, legend_label=r'$t\\bar{t}$')\n",
    "make_roc(zerobias_score, jetht_score, legend_label=r'Jet HT')\n",
    "make_roc(zerobias_score, softqcd_score, legend_label=r'Soft QCD')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfedd3-6831-4802-a2e1-de9d60a4cb4e",
   "metadata": {},
   "source": [
    "### Esoteric Losses\n",
    "\n",
    "This section is also a little more freeform. I just wanted to take some time to introduce other loss functions than Mean Squared Error here. Mean Squared Error is not a bad loss, and should be your first stop in checking things when making reconstruction losses, but it also can reward \"peak memorization\" and out of set reconstruction if you aren't careful. For images like we're working with, you do have other options. These are things I am experimenting around with, so I figured it might be interesting for you all to take a look at them as well and see how they work for this exercise. No guaratees from my side that these will be suitable or usable. It is good practice to try implementing your own loss functions though, because the loss function really does shape the behavior of a neural network as much as the layers you put in it you may be called to really try exploring that as much as you would trying to put in more or fancier layers.\n",
    "\n",
    "#### Huber Loss\n",
    "\n",
    "The [Huber Loss](https://keras.io/api/losses/regression_losses/#huber-class) comes from the [Huber Function](https://en.wikipedia.org/wiki/Huber_loss) a piecewise combination of Mean Absolute Error and Mean Squared Error. The idea behind it is to try and prevent giving outliers outsize importance by the squared portion of Mean Squared Error. It applies Mean Absolute Error to get most values pretty close but not give large outliers outsized importance, and then Mean Squared Error closer to 0 so that it hones in the accuracy of those values that form the non-outlier elements. Keras provides an implementation of this loss for us to use. \n",
    "\n",
    "Why would you try to be impervious to outliers in our anomaly detection autoencoder? Well, if it gives a larger loss to outliers, and learns to reconstruct them better to minimize it, we're baking in the ability to reconstruct anomlies with our loss function!\n",
    "\n",
    "#### Log-Cosh Loss\n",
    "\n",
    "The Log-Cosh Loss is another loss similar to the Huber loss, but is not piece-wise. It is calculated as $\\log{(e^{\\hat{y}-y}+e^{y-\\hat{y}})/2}$. It is designed to penalize outliers further out (the limit as the error grows large in either direction is functionally linear), and provide smooth gradients and training at all points. [Keras provides an implementation of this loss](https://keras.io/api/losses/regression_losses/#logcosh-class).\n",
    "\n",
    "#### Normalized Cross Correlation\n",
    "\n",
    "Normalized Cross Correlation is a metric between two signals or images (it doesn't necessarily have to be 2D) that measures the similarity of them, regardless of the amplitudes of the images. It rewards structural similarity over magnitude similarity. You can read a bit more [here](https://en.wikipedia.org/wiki/Cross-correlation#Zero-normalized_cross-correlation_(ZNCC)), but in general, think of it as a function that is close to 1 if the two patterns have the same shape (but not necessarily the same overall amplitude).\n",
    "\n",
    "It can be calculated like so:\n",
    "\n",
    "$\\frac{\\sum{((x-\\bar{x}) * (\\hat{x}-\\bar{\\bar{x}}))}}{\\sqrt{\\sum{x-\\bar{x}}^{2}} * \\sqrt{\\sum{\\hat{x}-\\bar{\\hat{x}}}^{2}}}$\n",
    "\n",
    "where $\\hat{x}$ is our predicted value, and $x$ is the true value.\n",
    "\n",
    "Since this grows with similarity instead of decreasing (i.e. it measures accuracy not error), but is bounded by 1.0, we can simply do $1-NCC$ as the actual loss function.\n",
    "\n",
    "#### Structural Similarity Index\n",
    "\n",
    "[Structural Similarity Index](https://en.wikipedia.org/wiki/Structural_similarity_index_measure) is a measure used in image transmission (television notably) to determine how similar a transmitted image looks to an observer. Rather than compare pizel by pizel though, it uses a lot of filtering and local structures to try and make a similarity judgement, even if the overall pixel values differ (In $MSE$ or $NCC$, an image shifted right by a few pixels could have a huge loss, by in structural similarity, it looks similar because the it maintains the variances and means roughly). The math behind it is a little more complicated, but nothing too terrible.\n",
    "\n",
    "Tensorflow as part of the various image processing utilities it has, has an implementation of this, but to use it, it requires a filter size for the rough sizes of structures to look out for, and a max value difference between pixels.\n",
    "\n",
    "#### Hybrid Losses\n",
    "\n",
    "Once you get used to using custom losses, nothing is also preventing you from _blending_ loss functions. $\\alpha*Loss_{1} + (1.0-\\alpha)*Loss_{2}$.\n",
    "\n",
    "I have included loss functions below for you to try out and look at. You may even notice that $MSE$ improves alongside some of these losses, or nothing prevents the two from being highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b738ec21-92d4-40a4-a469-394d1137c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions for you to use\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "huber_loss = keras.losses.Huber(delta=1.0)\n",
    "huber_metric = keras.losses.Huber(delta=1.0, reduction=None) # for use in evaluation later\n",
    "\n",
    "log_cosh_loss = keras.losses.LogCosh()\n",
    "log_cosh_metric = keras.losses.LogCosh(reduction=None)\n",
    "\n",
    "msle = keras.losses.MeanSquaredLogarithmicError()\n",
    "msle_metric = keras.losses.MeanSquaredLogarithmicError(reduction=None)\n",
    "\n",
    "def normalized_cross_correlation_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, dtype=tf.float64)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float64)\n",
    "\n",
    "    x_mean = tf.reshape(tf.reduce_mean(y_true, axis=(1,2,3)), [-1,1,1,1])\n",
    "    x_hat_mean = tf.reshape(tf.reduce_mean(y_pred, axis=(1,2,3)), [-1,1,1,1])\n",
    "\n",
    "    x_minus_mean = y_true-x_mean\n",
    "    x_hat_minus_mean = y_pred-x_hat_mean\n",
    "\n",
    "    num = tf.reduce_sum(x_minus_mean * x_hat_minus_mean, axis=(1,2,3))\n",
    "    denom = tf.math.sqrt(tf.reduce_sum(x_minus_mean**2, axis=(1,2,3)))*tf.math.sqrt(tf.reduce_sum(x_hat_minus_mean**2, axis=(1,2,3))) \n",
    "\n",
    "    return 1.0 - (num/(denom+1e-6))\n",
    "\n",
    "def ssim_loss_fn(y_true, y_pred, filter_size=5, max_val=1.0):\n",
    "    return 1.0 - tf.image.ssim(\n",
    "        tf.cast(y_true, dtype=tf.float64),\n",
    "        tf.cast(y_pred, dtype=tf.float64),\n",
    "        filter_size=filter_size,\n",
    "        max_val=max_val\n",
    "    )\n",
    "\n",
    "max_val_difference = np.max(zerobias_train) - np.min(zerobias_train)\n",
    "\n",
    "ssim_loss = lambda y_true, y_pred: ssim_loss_fn(y_true, y_pred, filter_size=5, max_val=max_val_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af467458-35fb-46a0-ba4f-3db2acf17694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_30          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_31          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_20            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_32          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d_5          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_33          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_21            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_10             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,216</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_34          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_22            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_11             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,504</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_35          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_23            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,005</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_30          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │            \u001b[38;5;34m20\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m3,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_31          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_20            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_32          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling2d_5          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m4,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_5 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_33          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_21            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_10             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m3,216\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_34          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_22            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_11             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m41,504\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_35          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout2d_23            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │         \u001b[38;5;34m4,005\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,961</span> (242.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,961\u001b[0m (242.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,743</span> (241.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61,743\u001b[0m (241.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">218</span> (872.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m218\u001b[0m (872.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - loss: 0.0353 - mae: 0.0931 - mse: 1.4270 - val_loss: 0.0144 - val_mae: 0.0214 - val_mse: 2.4774\n",
      "Epoch 2/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0147 - mae: 0.0270 - mse: 1.7685 - val_loss: 0.0143 - val_mae: 0.0184 - val_mse: 2.4761\n",
      "Epoch 3/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0141 - mae: 0.0219 - mse: 1.7320 - val_loss: 0.0142 - val_mae: 0.0182 - val_mse: 2.4735\n",
      "Epoch 4/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0140 - mae: 0.0188 - mse: 1.7035 - val_loss: 0.0142 - val_mae: 0.0171 - val_mse: 2.4745\n",
      "Epoch 5/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0140 - mae: 0.0176 - mse: 1.5546 - val_loss: 0.0142 - val_mae: 0.0168 - val_mse: 2.4748\n",
      "Epoch 6/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0141 - mae: 0.0174 - mse: 1.9375 - val_loss: 0.0142 - val_mae: 0.0179 - val_mse: 2.4743\n",
      "Epoch 7/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0144 - mae: 0.0178 - mse: 2.4925 - val_loss: 0.0142 - val_mae: 0.0176 - val_mse: 2.4743\n",
      "Epoch 8/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0140 - mae: 0.0180 - mse: 1.8268 - val_loss: 0.0141 - val_mae: 0.0180 - val_mse: 2.4725\n",
      "Epoch 9/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0138 - mae: 0.0188 - mse: 1.4413 - val_loss: 0.0138 - val_mae: 0.0181 - val_mse: 2.4621\n",
      "Epoch 10/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0135 - mae: 0.0197 - mse: 1.5471 - val_loss: 0.0132 - val_mae: 0.0174 - val_mse: 2.4492\n",
      "Epoch 11/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0134 - mae: 0.0196 - mse: 1.9145 - val_loss: 0.0129 - val_mae: 0.0179 - val_mse: 2.4410\n",
      "Epoch 12/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0128 - mae: 0.0190 - mse: 1.1741 - val_loss: 0.0128 - val_mae: 0.0172 - val_mse: 2.4391\n",
      "Epoch 13/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0129 - mae: 0.0189 - mse: 1.5199 - val_loss: 0.0126 - val_mae: 0.0167 - val_mse: 2.4365\n",
      "Epoch 14/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0129 - mae: 0.0186 - mse: 2.1415 - val_loss: 0.0126 - val_mae: 0.0164 - val_mse: 2.4349\n",
      "Epoch 15/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0128 - mae: 0.0184 - mse: 1.4332 - val_loss: 0.0127 - val_mae: 0.0168 - val_mse: 2.4365\n",
      "Epoch 16/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0127 - mae: 0.0183 - mse: 1.6400 - val_loss: 0.0125 - val_mae: 0.0171 - val_mse: 2.4325\n",
      "Epoch 17/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0127 - mae: 0.0183 - mse: 1.4913 - val_loss: 0.0124 - val_mae: 0.0160 - val_mse: 2.4316\n",
      "Epoch 18/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0124 - mae: 0.0180 - mse: 1.4686 - val_loss: 0.0124 - val_mae: 0.0174 - val_mse: 2.4293\n",
      "Epoch 19/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0182 - mse: 1.4931 - val_loss: 0.0126 - val_mae: 0.0162 - val_mse: 2.4351\n",
      "Epoch 20/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 0.0126 - mae: 0.0181 - mse: 1.6589 - val_loss: 0.0124 - val_mae: 0.0166 - val_mse: 2.4303\n",
      "Epoch 21/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0179 - mse: 1.0254 - val_loss: 0.0126 - val_mae: 0.0175 - val_mse: 2.4346\n",
      "Epoch 22/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0124 - mae: 0.0179 - mse: 1.7036 - val_loss: 0.0124 - val_mae: 0.0161 - val_mse: 2.4310\n",
      "Epoch 23/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0180 - mse: 1.4994 - val_loss: 0.0125 - val_mae: 0.0167 - val_mse: 2.4311\n",
      "Epoch 24/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0177 - mse: 1.4607 - val_loss: 0.0123 - val_mae: 0.0168 - val_mse: 2.4282\n",
      "Epoch 25/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0126 - mae: 0.0179 - mse: 1.5547 - val_loss: 0.0125 - val_mae: 0.0161 - val_mse: 2.4327\n",
      "Epoch 26/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0179 - mse: 1.9894 - val_loss: 0.0126 - val_mae: 0.0164 - val_mse: 2.4337\n",
      "Epoch 27/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0178 - mse: 1.4684 - val_loss: 0.0124 - val_mae: 0.0162 - val_mse: 2.4301\n",
      "Epoch 28/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0128 - mae: 0.0181 - mse: 2.9728 - val_loss: 0.0124 - val_mae: 0.0164 - val_mse: 2.4312\n",
      "Epoch 29/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0124 - mae: 0.0177 - mse: 1.6195 - val_loss: 0.0124 - val_mae: 0.0182 - val_mse: 2.4288\n",
      "Epoch 30/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0126 - mae: 0.0180 - mse: 1.6058 - val_loss: 0.0123 - val_mae: 0.0162 - val_mse: 2.4288\n",
      "Epoch 31/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0176 - mse: 1.3531 - val_loss: 0.0124 - val_mae: 0.0162 - val_mse: 2.4300\n",
      "Epoch 32/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0178 - mse: 1.9117 - val_loss: 0.0123 - val_mae: 0.0172 - val_mse: 2.4283\n",
      "Epoch 33/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0124 - mae: 0.0177 - mse: 1.5648 - val_loss: 0.0123 - val_mae: 0.0169 - val_mse: 2.4262\n",
      "Epoch 34/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0179 - mse: 1.7826 - val_loss: 0.0124 - val_mae: 0.0161 - val_mse: 2.4298\n",
      "Epoch 35/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0124 - mae: 0.0177 - mse: 1.6216 - val_loss: 0.0126 - val_mae: 0.0160 - val_mse: 2.4332\n",
      "Epoch 36/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0178 - mse: 1.8848 - val_loss: 0.0124 - val_mae: 0.0161 - val_mse: 2.4290\n",
      "Epoch 37/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0126 - mae: 0.0181 - mse: 1.6465 - val_loss: 0.0124 - val_mae: 0.0173 - val_mse: 2.4294\n",
      "Epoch 38/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0180 - mse: 1.8780 - val_loss: 0.0124 - val_mae: 0.0184 - val_mse: 2.4280\n",
      "Epoch 39/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0179 - mse: 1.9776 - val_loss: 0.0122 - val_mae: 0.0159 - val_mse: 2.4269\n",
      "Epoch 40/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0124 - mae: 0.0181 - mse: 1.7660 - val_loss: 0.0121 - val_mae: 0.0166 - val_mse: 2.4225\n",
      "Epoch 41/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0122 - mae: 0.0179 - mse: 1.1064 - val_loss: 0.0123 - val_mae: 0.0173 - val_mse: 2.4249\n",
      "Epoch 42/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0122 - mae: 0.0178 - mse: 1.4212 - val_loss: 0.0122 - val_mae: 0.0159 - val_mse: 2.4235\n",
      "Epoch 43/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0180 - mse: 1.3586 - val_loss: 0.0123 - val_mae: 0.0165 - val_mse: 2.4223\n",
      "Epoch 44/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0179 - mse: 1.8656 - val_loss: 0.0121 - val_mae: 0.0159 - val_mse: 2.4211\n",
      "Epoch 45/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0178 - mse: 1.3736 - val_loss: 0.0121 - val_mae: 0.0160 - val_mse: 2.4220\n",
      "Epoch 46/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0120 - mae: 0.0177 - mse: 1.6353 - val_loss: 0.0122 - val_mae: 0.0167 - val_mse: 2.4210\n",
      "Epoch 47/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0181 - mse: 2.9516 - val_loss: 0.0122 - val_mae: 0.0164 - val_mse: 2.4192\n",
      "Epoch 48/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0122 - mae: 0.0179 - mse: 1.9705 - val_loss: 0.0120 - val_mae: 0.0161 - val_mse: 2.4171\n",
      "Epoch 49/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0120 - mae: 0.0175 - mse: 1.1529 - val_loss: 0.0121 - val_mae: 0.0159 - val_mse: 2.4174\n",
      "Epoch 50/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0178 - mse: 1.7370 - val_loss: 0.0122 - val_mae: 0.0161 - val_mse: 2.4218\n",
      "Epoch 51/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0179 - mse: 1.6242 - val_loss: 0.0120 - val_mae: 0.0161 - val_mse: 2.4164\n",
      "Epoch 52/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0122 - mae: 0.0178 - mse: 2.1347 - val_loss: 0.0122 - val_mae: 0.0158 - val_mse: 2.4181\n",
      "Epoch 53/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0125 - mae: 0.0181 - mse: 3.0975 - val_loss: 0.0123 - val_mae: 0.0160 - val_mse: 2.4233\n",
      "Epoch 54/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0176 - mse: 2.0066 - val_loss: 0.0122 - val_mae: 0.0177 - val_mse: 2.4211\n",
      "Epoch 55/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0120 - mae: 0.0175 - mse: 1.2981 - val_loss: 0.0121 - val_mae: 0.0158 - val_mse: 2.4189\n",
      "Epoch 56/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0122 - mae: 0.0178 - mse: 1.5872 - val_loss: 0.0121 - val_mae: 0.0160 - val_mse: 2.4194\n",
      "Epoch 57/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0122 - mae: 0.0178 - mse: 2.8364 - val_loss: 0.0122 - val_mae: 0.0163 - val_mse: 2.4181\n",
      "Epoch 58/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0180 - mse: 1.7944 - val_loss: 0.0122 - val_mae: 0.0162 - val_mse: 2.4181\n",
      "Epoch 59/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0178 - mse: 1.2453 - val_loss: 0.0121 - val_mae: 0.0166 - val_mse: 2.4185\n",
      "Epoch 60/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0179 - mse: 2.1650 - val_loss: 0.0120 - val_mae: 0.0157 - val_mse: 2.4163\n",
      "Epoch 61/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0120 - mae: 0.0177 - mse: 1.2177 - val_loss: 0.0120 - val_mae: 0.0160 - val_mse: 2.4185\n",
      "Epoch 62/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0118 - mae: 0.0174 - mse: 1.0249 - val_loss: 0.0120 - val_mae: 0.0157 - val_mse: 2.4163\n",
      "Epoch 63/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.0122 - mae: 0.0179 - mse: 2.4812 - val_loss: 0.0121 - val_mae: 0.0159 - val_mse: 2.4197\n",
      "Epoch 64/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0180 - mse: 1.4714 - val_loss: 0.0120 - val_mae: 0.0156 - val_mse: 2.4170\n",
      "Epoch 65/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0177 - mse: 1.3583 - val_loss: 0.0120 - val_mae: 0.0162 - val_mse: 2.4166\n",
      "Epoch 66/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0123 - mae: 0.0180 - mse: 2.0120 - val_loss: 0.0121 - val_mae: 0.0168 - val_mse: 2.4151\n",
      "Epoch 67/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0177 - mse: 1.8683 - val_loss: 0.0121 - val_mae: 0.0159 - val_mse: 2.4163\n",
      "Epoch 68/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.0120 - mae: 0.0176 - mse: 1.3698 - val_loss: 0.0122 - val_mae: 0.0165 - val_mse: 2.4200\n",
      "Epoch 69/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0120 - mae: 0.0176 - mse: 1.7664 - val_loss: 0.0120 - val_mae: 0.0155 - val_mse: 2.4156\n",
      "Epoch 70/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0177 - mse: 1.4869 - val_loss: 0.0120 - val_mae: 0.0159 - val_mse: 2.4196\n",
      "Epoch 71/200\n",
      "\u001b[1m2187/2187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 0.0121 - mae: 0.0176 - mse: 1.8451 - val_loss: 0.0120 - val_mae: 0.0158 - val_mse: 2.4177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fbc3012f4c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Exercise: Make a seperate model with a different loss function\n",
    "#\n",
    "\n",
    "new_ae = keras.Sequential([\n",
    "    #\n",
    "    # encoder\n",
    "    #\n",
    "    keras.layers.Input(shape=zerobias_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=7,\n",
    "        padding='same',\n",
    "        activation='leaky_relu',\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    #Size is now 8 x 8, 16 filters\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "    keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        activation='leaky_relu'\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.GlobalMaxPooling2D(),\n",
    "    #\n",
    "    # Final latent space size and shape: 32 flat entries\n",
    "    #\n",
    "    #\n",
    "    # decoder\n",
    "    #\n",
    "    keras.layers.Dense(4*4*8, activation='leaky_relu'),\n",
    "    keras.layers.Reshape((4,4,8)), #new shape is 4 x 4 with 8 features\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    #keras.layers.Conv2DTranspose(16, kernel_size=2, strides=2, activation='leaky_relu'), # new shape is 8 x 8 with 16 features\n",
    "    keras.layers.Conv2DTranspose(16, kernel_size=5, strides=1, activation='leaky_relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    #keras.layers.Conv2DTranspose(32, kernel_size=2, strides=2, activation='leaky_relu'), # new shape is 16 x 16 with 32 features\n",
    "    keras.layers.Conv2DTranspose(32, kernel_size=9, strides=1, activation='leaky_relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    keras.layers.Conv2D(5, kernel_size=5, padding='same'), # final shape is 16 x 16 with 5 features\n",
    "])\n",
    "\n",
    "new_ae.compile(\n",
    "    optimizer='adam',\n",
    "    #loss=huber_loss,\n",
    "    #loss=normalized_cross_correlation_loss,\n",
    "    #loss=ssim_loss,\n",
    "    loss=log_cosh_loss,\n",
    "    metrics=['mse', 'mae'],\n",
    ")\n",
    "new_ae.summary()\n",
    "\n",
    "new_ae.fit(\n",
    "    x=zerobias_data,\n",
    "    y=zerobias_data,\n",
    "    validation_data=(zerobias_val, zerobias_val),\n",
    "    epochs=200,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85407e8e-674d-493c-a758-ddb9ea4a8865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0117 - mae: 0.0157 - mse: 1.2126\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.011452150531113148, 1.1258525848388672, 0.015387937426567078]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ae.evaluate(zerobias_test, zerobias_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f289a9-e534-4f49-b238-48f913299412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m2779/2779\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step\n",
      "\u001b[1m2032/2032\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "\u001b[1m3856/6387\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Exercise: Compare this different loss function model to the previous model. Which works better?\n",
    "#\n",
    "\n",
    "new_zerobias_predictions = conv_ae.predict(zerobias_test)\n",
    "new_ttbar_predictions = conv_ae.predict(ttbar_data)\n",
    "new_jetht_predictions = conv_ae.predict(jetht_data)\n",
    "new_softqcd_predictions = conv_ae.predict(softqcd_data)\n",
    "\n",
    "test = huber_metric(zerobias_test, new_zerobias_predictions)\n",
    "console.print(new_zerobias_predictions.shape)\n",
    "console.print(test.shape)\n",
    "\n",
    "#new_zerobias_score = np.mean(np.array(huber_metric(zerobias_test, new_zerobias_predictions)), axis=(1,2))\n",
    "#new_ttbar_score = np.mean(np.array(huber_metric(ttbar_data, new_ttbar_predictions)), axis=(1,2))\n",
    "#new_jetht_score = np.mean(np.array(huber_metric(jetht_data, new_jetht_predictions)), axis=(1,2))\n",
    "#new_softqcd_score = np.mean(np.array(huber_metric(softqcd_data, new_softqcd_predictions)), axis=(1,2))\n",
    "\n",
    "new_zerobias_score = np.mean(np.array(log_cosh_metric(zerobias_test, new_zerobias_predictions)), axis=(1,2))\n",
    "new_ttbar_score = np.mean(np.array(log_cosh_metric(ttbar_data, new_ttbar_predictions)), axis=(1,2))\n",
    "new_jetht_score = np.mean(np.array(log_cosh_metric(jetht_data, new_jetht_predictions)), axis=(1,2))\n",
    "new_softqcd_score = np.mean(np.array(log_cosh_metric(softqcd_data, new_softqcd_predictions)), axis=(1,2))\n",
    "\n",
    "#new_zerobias_score = normalized_cross_correlation_loss(zerobias_test, np.array(new_zerobias_predictions))\n",
    "#new_ttbar_score = normalized_cross_correlation_loss(ttbar_data, np.array(new_ttbar_predictions))\n",
    "#new_jetht_score = normalized_cross_correlation_loss(jetht_data, np.array(new_jetht_predictions))\n",
    "#new_softqcd_score = normalized_cross_correlation_loss(softqcd_data, np.array(new_softqcd_predictions))\n",
    "\n",
    "#new_zerobias_score = ssim_loss(zerobias_test, np.array(new_zerobias_predictions))\n",
    "#new_ttbar_score = ssim_loss(ttbar_data, np.array(new_ttbar_predictions))\n",
    "#new_jetht_score = ssim_loss(jetht_data, np.array(new_jetht_predictions))\n",
    "#new_softqcd_score = ssim_loss(softqcd_data, np.array(new_softqcd_predictions))\n",
    "\n",
    "make_roc(new_zerobias_score, new_ttbar_score, legend_label=r'$t\\bar{t}$')\n",
    "make_roc(new_zerobias_score, new_jetht_score, legend_label=r'Jet HT')\n",
    "make_roc(new_zerobias_score, new_softqcd_score, legend_label=r'Soft QCD')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9c556-3fed-4fb0-977b-fefa7db32d72",
   "metadata": {},
   "source": [
    "### Different Networks for Anomaly Detection: Generative Adversarial Networks\n",
    "Up to this point we have been focusing exclusively on different types of Autoencoder, because they form the most common and easiest to use idea in anomaly detection. They are not the only unsupervised technique in neural networks though, which means they are not the only technique in anomaly detection. The other big kind of network is the \"Generative Adversarial Network\". The idea goes like this: Generative Adversarial Networks actually use a pair of models, the first model is designed to take a bunch of noise/random numbers, and from this, it creates data that looks like the input dataset you are doing unsupervised learning on. The second model is a classifier, and its job is to judge whether the images it has gotten are genuinely from the dataset, or are fakes. The first model is trained to try and fool the second classifier model, the second classifier model is trained to try and find the genuine article and not get fooled.\n",
    "\n",
    "At the end of training both of these models, the classifier then should be relatively picky and serve as a good judge of whether something belongs to the dataset it has seen, or is anomalous.\n",
    "\n",
    "Because the training loop is a bit particular and odd, we can't actually use the standard `fit` function. I will provide the training loop, you provide the models. This training loop is also _very_ slow due to the amount of things that have to go on to get it to work (EDIT: Potentially also some memory issues with this?). It likely will not finish in the time we have. There is probably some optimization of things that happen here to make it a little less \"by hand\", but it should showcase the basic idea, which is what I want you to have. You should know that you can do anomaly detection with other neural networks, and there may even be some upsides to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "665b14df-76ac-4c0c-a3ae-bbef36f9a963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8737202d6f7438fb742efb55ff18551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 11:40:56.286464: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.7 = (f32[64,16,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[64,8,5,5]{3,2,1,0} %bitcast.11344, f32[16,8,3,3]{3,2,1,0} %bitcast.10394, f32[16]{0} %bitcast.11414), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_5_1/conv2d_13_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "2025-06-13 11:41:00.300455: I external/local_xla/xla/service/gpu/autotuning/conv_algorithm_picker.cc:549] Omitted potentially buggy algorithm eng14{k25=0} for conv %cudnn-conv-bias-activation.7 = (f32[32,16,3,3]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,8,5,5]{3,2,1,0} %bitcast.11928, f32[16,8,3,3]{3,2,1,0} %bitcast.11935, f32[16]{0} %bitcast.11937), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"sequential_6_1/sequential_5_1/conv2d_13_1/convolution\" source_file=\"/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer._make_function.<locals>.one_step_on_data at 0x7f9ba7c653a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer._make_function.<locals>.one_step_on_data at 0x7f9ba5204160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m             gan\u001b[38;5;241m.\u001b[39mtrain_on_batch(noise, y2)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gan\n\u001b[0;32m--> 122\u001b[0m trained_gan \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzerobias_gan_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_coding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#The classifier and generator can be split off into seperate models\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#The classifier can be used for anomaly detection by evaluating on datasets as usual\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#Depending on what you want to do, you can find a use for the generator too.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 119\u001b[0m, in \u001b[0;36mtrain_gan\u001b[0;34m(gan, dataset, input_coding_size, n_epochs, batch_size)\u001b[0m\n\u001b[1;32m    117\u001b[0m         classifier\u001b[38;5;241m.\u001b[39mtrainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    118\u001b[0m         gan\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m         \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gan\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:605\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdata\u001b[39m():\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[0;32m--> 605\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:228\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution), iterator\n\u001b[1;32m    227\u001b[0m     ):\n\u001b[0;32m--> 228\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:906\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    910\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[1;32m    911\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[1;32m    912\u001b[0m   )\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m/afs/hep.wisc.edu/cms/aloeliger/anomalyTriggerWork/ADWorkshop/CMS_OpenData_Exercises/cms_opendata_exercises_env/lib64/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Exercise: Fill in the generator_model and classifier_model\n",
    "#\n",
    "from rich.progress import track\n",
    "\n",
    "#Fill this in\n",
    "#The generator model will need to take some random amount of noise as inputs\n",
    "#But end up in our 16 x 16 with 5 features outputs\n",
    "# I chose a flat input of size 4 * 4 * 2 = 32 noise inputs, that I reshape into 4 x 4 with 2 features\n",
    "generator_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(32,)),\n",
    "    keras.layers.Reshape((4,4,2)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Conv2DTranspose(4, kernel_size=2, strides=2, activation='leaky_relu'), # 8 x 8 with 4 features\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "    keras.layers.Conv2DTranspose(5, kernel_size=2, strides=2) # 16 x 16 with 5 features\n",
    "])\n",
    "\n",
    "#This will run on the 16 x 16 with 5 features present in zero bias data\n",
    "# or the generator model output\n",
    "classifier_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=zerobias_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(\n",
    "        8,\n",
    "        kernel_size=7,\n",
    "        activation='leaky_relu',\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    keras.layers.Conv2D(\n",
    "        16,\n",
    "        kernel_size=3,\n",
    "        activation='leaky_relu',\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.SpatialDropout2D(0.2),\n",
    "\n",
    "    keras.layers.GlobalMaxPooling2D(),\n",
    "    keras.layers.Dense(8, activation='leaky_relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "batch_size=32\n",
    "zerobias_gan_dataset = tf.data.Dataset.from_tensor_slices(zerobias_train).shuffle(1000)\n",
    "zerobias_gan_dataset = zerobias_gan_dataset.batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "#TF can be a little funny about types it allows in training. Our dataset is double, it wants float\n",
    "#This just quickly converts it\n",
    "def cast_to_tf_float(x):\n",
    "    return tf.cast(x, tf.dtypes.float32)\n",
    "zerobias_gan_dataset = zerobias_gan_dataset.map(cast_to_tf_float)\n",
    "\n",
    "gan = keras.models.Sequential([generator_model, classifier_model])\n",
    "\n",
    "classifier_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "def train_gan(gan, dataset, input_coding_size, n_epochs=5, batch_size=32):\n",
    "    generator, classifier = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        console.print(f'Epoch: {epoch}')\n",
    "        for x_batch in track(dataset):\n",
    "            #Random noise to feed to the generator model\n",
    "            noise = tf.random.normal(shape=[batch_size, input_coding_size])\n",
    "            #First we train the classifier model by itself. To do that, we need some generated images\n",
    "            #We generate that with the generator model and some noise. These won't be good at first\n",
    "            #But we'll train the generator to fool the classifier soon\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "            #concatenate our fake images and genuine images together\n",
    "            x_fake_and_real = tf.concat([generated_images, x_batch], axis=0)\n",
    "            #Now we need our labels, beause we're training the classifier, the noise images should be 0, the genuine should be 1\n",
    "            y1 = tf.concat([\n",
    "                    tf.zeros((batch_size,)),\n",
    "                    tf.ones((batch_size,)),\n",
    "                ],\n",
    "                axis=0\n",
    "            )\n",
    "            # Note, we could also provide labels that are not-directly 0 or 1, but close, like so: \n",
    "            # y1 = tf.concat(                                                                                                                            \n",
    "            #   [                                                                                                                                      \n",
    "            #    tf.math.abs(                                                                                                                       \n",
    "            #       tf.random.normal(                                                                                                              \n",
    "            #          mean=0.0,                                                                                                                  \n",
    "            #          stddev = 0.1,                                                                                                              \n",
    "            #          shape = [batch_size,]                                                                                                      \n",
    "            #       )                                                                                                                              \n",
    "            #    ),                                                                                                                                 \n",
    "            #    1.0-tf.math.abs(                                                                                                                   \n",
    "            #        tf.random.normal(                                                                                                              \n",
    "            #          mean=0.0,                                                                                                                  \n",
    "            #          stddev = 0.1,                                                                                                              \n",
    "            #          shape = [batch_size,]                                                                                                      \n",
    "            #        )                                                                                                                              \n",
    "            #    ),                                                                                                                                 \n",
    "            #   ],                                                                                                                                     \n",
    "            #   axis=0,                                                                                                                                \n",
    "            # )                          \n",
    "            #in order to discourage the network from collapsing into poor equilibria. I'll talk about that in the next section\n",
    "            #make sure the classifier is trainable (it will be deactivated later, so each loop we want to activate it again)\n",
    "            classifier.trainable=True\n",
    "            classifier.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "            #Actually train the classifier on the batch after all the setup!\n",
    "            classifier.train_on_batch(x_fake_and_real, y1)\n",
    "\n",
    "            #Okay, now we need to train the generator, the idea is, when we feed the entire generator+classifier chain noise,\n",
    "            #The classifier should rate everything a 1, i.e. we think the fake images are genuine data\n",
    "            #So generate some new noise, and 1 labels\n",
    "            noise = tf.random.normal(shape=[batch_size, input_coding_size])\n",
    "            y2 = tf.ones((batch_size,))\n",
    "            # Now we need to avoid training the classifier while trying to train the generator\n",
    "            # We don't want to train it one direction then the opposite\n",
    "            classifier.trainable=False\n",
    "            gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "            gan.train_on_batch(noise, y2)\n",
    "    return gan\n",
    "\n",
    "trained_gan = train_gan(gan, zerobias_gan_dataset, input_coding_size=32, n_epochs=1, batch_size=32)\n",
    "\n",
    "#The classifier and generator can be split off into seperate models\n",
    "#The classifier can be used for anomaly detection by evaluating on datasets as usual\n",
    "#Depending on what you want to do, you can find a use for the generator too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2874f8e-5b45-452d-aecf-fc2b2972b515",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks are an interesting idea, and have some great upsides (a direct classifier trained to do almost exactly the task we are after, a free model that does generative stuff, etc.) but of course they are not without their downsides. The big one is the training loop, which has some problems that stem from game theory. Because these two networks are competing, they can end up in what is called a [Nash Equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium). A Nash Equilibrium is a scenario where neither network stands to gain from changing anymore, if the generator changes it will not fool the classifier any more than it already does, and potentially will become worse, and the classifier cannot be any better on the data it is seeing than it already is. Note, that being in this state does not guarantee that the generated images are any good, nor that the classifier is very accurate to our underlying dataset! This is also called a \"mode collapse\"\n",
    "\n",
    "There are some techniques to try and get around this. One is [SpectralNormalization](https://keras.io/api/layers/preprocessing_layers/numerical/spectral_normalization/) of layers. Another is adding some \"fuzz\" or \"jitter\" to the labels of the dataset (i.e. not using 0 or 1, but removing or adding a tiny bit to each) to try and force the networks out of stable states.\n",
    "\n",
    "We won't have time to explore them here. That's homework, as is checking out the classifier's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add8897-d7d2-47ff-aa18-9bd7dcb5d837",
   "metadata": {},
   "source": [
    "### Wrap up\n",
    "\n",
    "Barring some kind of LLM based autoencoder, we've covered a lot of the basics of most of the modern anomaly detection neural network methods. The only remaining major technique to cover is Graph Neural Networks. We'll talk about those in the next exercise if we get time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
