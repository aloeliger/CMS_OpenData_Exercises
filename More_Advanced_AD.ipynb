{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4368ea-3569-44db-990d-f2bc4ced8250",
   "metadata": {},
   "source": [
    "# Exercise 3: More Advanced AD with Convolutional Auto-Encoders, Non-Standard Losses, And Generative Adversarial Network Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10933f-724b-4837-90f1-81494e1effd8",
   "metadata": {},
   "source": [
    "### Goals of the Exercise\n",
    "\n",
    "- Make a 2D convolutional autoencoder for events\n",
    "- Explore some non-standard losses for 2D images/autoencoders\n",
    "- Create a Generative Adversarial Network for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aff670-c1f8-4d6d-bf68-5f832fd0fc29",
   "metadata": {},
   "source": [
    "### More Anomaly Detection With Autoencoders\n",
    "\n",
    "A very common form of data in particle physics (and everything else) is images. 2D maps of the visual of something. There are, of course, autoencoders for this, \"convolutional autoencoders\", that use the same filter based features as a traditional image classifier convolutional neural network.\n",
    "\n",
    "Very often, data comes in 2D grids (especially for something like a detector or calorimeter), or a 2D image can be made by imposing a grid based structure on spatial data (note: this is acutally a bit non-ideal as it can lead to sparse data representations, and a whole lot of math on a whole lot of nothing, a problem that graph auto-encoders in the next exercise solve).\n",
    "\n",
    "at this point, you know the drill, we're going to make some auto-encoders for this, and see how they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f02ca4-8fbe-4202-a06a-d085fb508949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12cfedd3-6831-4802-a2e1-de9d60a4cb4e",
   "metadata": {},
   "source": [
    "### Esoteric Losses\n",
    "\n",
    "This section is a little more freeform. I just wanted to take some time to introduce other loss functions than Mean Squared Error here. Mean Squared Error is not a bad loss, and should be your first stop in checking things when making reconstruction losses, but it also can reward \"peak memorization\" and out of set reconstruction if you aren't careful. For images like we're working with, you do have other options. These are things I am experimenting around with, so I figured it might be interesting for you all to take a look at them as well and see how they work for this exercise. No guaratees from my side that these will be suitable or usable. It is good practice to try implementing your own loss functions though, because the loss function really does shape the behavior of a neural network as much as the layers you put in it you may be called to really try exploring that as much as you would trying to put in more or fancier layers.\n",
    "\n",
    "##### Huber Loss\n",
    "\n",
    "##### Normalized Cross Correlation\n",
    "\n",
    "##### Structural Similarity Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738ec21-92d4-40a4-a469-394d1137c1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece9c556-3fed-4fb0-977b-fefa7db32d72",
   "metadata": {},
   "source": [
    "### Different Networks for Anomaly Detection: Generative Adversarial Networks\n",
    "Up to this point we have been focusing excluively on different types of Autoencoder, because they form the most common and easiest to use idea in anomaly detection. They are not the only unsupervised technique in neural networks though, which means they are not the only technique in anomaly detection. The other big kind of network is the \"Generative Adversarial Network\". The idea goes like this: Generative Adversarial Networks actually use a pair of models, the first model is designed to take a bunch of noise/random numbers, and from this, it creates data that looks like the input dataset you are doing unsupervised learning on. The second model is a classifier, and its job is to judge whether the images it has gotten are genuinely from the dataset, or are fakes. The first model is trained to try and fool the second classifier model, the second classifier model is trained to try and find the genuine article and not get fooled.\n",
    "\n",
    "At the end of training both of these models, the classifier then should be relatively picky and serve as a good judge of whether something belongs to the dataset it has seen, or is anomalous.\n",
    "\n",
    "Because the training loop is a bit odd, we can't actually use the standard `fit` function. I will provide the training loop, you provide the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665b14df-76ac-4c0c-a3ae-bbef36f9a963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2874f8e-5b45-452d-aecf-fc2b2972b515",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks are an interesting idea, and have some great upsides (a direct classifier trained to do almost exactly the task we are after, a free model that does generative stuff, etc.) but of course they are not without their downsides. The big one is the training loop, which has some problems that stem from game theory. Because these two networks are competing, they can end up in what is called a [Nash Equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium). A Nash Equilibrium is a scenario where neither network stands to gain from changing anymore, if the generator changes it will not fool the classifier any more than it already does, and potentially will become worse, and the classifier cannot be any better on the data it is seeing than it already is. Note, that being in this state does not guarantee that the generated images are any good, nor that the classifier is very accurate to our underlying dataset!\n",
    "\n",
    "There are some techniques to try and get around this. One is [SpectralNormalization](https://keras.io/api/layers/preprocessing_layers/numerical/spectral_normalization/) of layers. Another is adding some \"fuzz\" or \"jitter\" to the labels of the dataset (i.e. not using 0 or 1, but removing or adding a tiny bit to each) to try and force the networks out of stable states.\n",
    "\n",
    "We won't have time to explore them here. That's homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add8897-d7d2-47ff-aa18-9bd7dcb5d837",
   "metadata": {},
   "source": [
    "### Wrap up\n",
    "\n",
    "Barring some kind of LLM based autoencoder, we've covered a lot of the basics of most of the modern anomaly detection neural network methods. The only remaining major technique to cover is Graph Neural Networks. We'll talk about those in the next exercise if we get time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4aae1-a79a-4ae9-a0cf-1058e05dd46c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
